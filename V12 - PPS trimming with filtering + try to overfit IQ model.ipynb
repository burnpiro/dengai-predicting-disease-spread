{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import functools\n",
    "from IPython.display import Image, clear_output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from data_info import *\n",
    "from preprocessing_helpers import *\n",
    "from datetime import datetime\n",
    "%matplotlib inline\n",
    "pd.set_option(\"display.precision\", 2)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_iq_norm = [\n",
    "                'precipitation_amt_mm',\n",
    "                'reanalysis_air_temp_k',\n",
    "                'reanalysis_avg_temp_k',\n",
    "                'reanalysis_dew_point_temp_k',\n",
    "                'reanalysis_max_air_temp_k',\n",
    "                'reanalysis_min_air_temp_k',\n",
    "                'reanalysis_precip_amt_kg_per_m2',\n",
    "                'reanalysis_sat_precip_amt_mm',\n",
    "                'reanalysis_specific_humidity_g_per_kg',\n",
    "                'reanalysis_tdtr_k',\n",
    "                'station_avg_temp_c',\n",
    "                'station_diur_temp_rng_c',\n",
    "                'station_max_temp_c',\n",
    "                'station_min_temp_c',\n",
    "]\n",
    "new_iq_scale = [\n",
    "                 'weekofyear',\n",
    "]\n",
    "\n",
    "extra_iq_cols = [\n",
    "]\n",
    "new_sj_norm = [\n",
    "                'reanalysis_air_temp_k',\n",
    "                'reanalysis_avg_temp_k',\n",
    "                'reanalysis_dew_point_temp_k',\n",
    "                'reanalysis_max_air_temp_k',\n",
    "                'reanalysis_min_air_temp_k',\n",
    "                'reanalysis_precip_amt_kg_per_m2',\n",
    "                'reanalysis_relative_humidity_percent',\n",
    "                'reanalysis_sat_precip_amt_mm',\n",
    "                'reanalysis_specific_humidity_g_per_kg',\n",
    "                'station_avg_temp_c',\n",
    "                'station_diur_temp_rng_c',\n",
    "                'station_max_temp_c',\n",
    "                'station_min_temp_c',\n",
    "]\n",
    "new_sj_scale = [\n",
    "                 'weekofyear',\n",
    "]\n",
    "\n",
    "extra_sj_cols = [\n",
    "]\n",
    "new_iq_cols = [LABEL_COLUMN] + CATEGORICAL_COLUMNS + new_iq_norm + new_iq_scale + extra_iq_cols + [DATETIME_COLUMN]\n",
    "new_iq_cols_no_label = CATEGORICAL_COLUMNS + new_iq_norm + new_iq_scale + extra_iq_cols + [DATETIME_COLUMN]\n",
    "new_sj_cols = [LABEL_COLUMN] + CATEGORICAL_COLUMNS + new_sj_norm + new_sj_scale + extra_sj_cols + [DATETIME_COLUMN]\n",
    "new_sj_cols_no_label = CATEGORICAL_COLUMNS + new_sj_norm + new_sj_scale + extra_sj_cols + [DATETIME_COLUMN]\n",
    "\n",
    "\n",
    "sj_col_size = {\n",
    "    'precipitation_amt_mm': 40,\n",
    "    'reanalysis_air_temp_k': 16,\n",
    "    'reanalysis_avg_temp_k': 15,\n",
    "    'reanalysis_dew_point_temp_k': 39,\n",
    "    'reanalysis_max_air_temp_k': 12,\n",
    "    'reanalysis_min_air_temp_k': 21,\n",
    "    'reanalysis_precip_amt_kg_per_m2': 30,\n",
    "    'reanalysis_relative_humidity_percent': 34,\n",
    "    'reanalysis_sat_precip_amt_mm': 40,\n",
    "    'reanalysis_specific_humidity_g_per_kg': 14,\n",
    "    'reanalysis_tdtr_k': 21,\n",
    "    'station_avg_temp_c': 41,\n",
    "    'station_diur_temp_rng_c': 40,\n",
    "    'station_max_temp_c': 37,\n",
    "    'station_min_temp_c': 26,\n",
    "    'station_precip_mm': 32,\n",
    "    'weekofyear': 3\n",
    "}\n",
    "iq_col_size = {\n",
    "    'precipitation_amt_mm': 33,\n",
    "    'reanalysis_air_temp_k': 10,\n",
    "    'reanalysis_avg_temp_k': 4,\n",
    "    'reanalysis_dew_point_temp_k': 6,\n",
    "    'reanalysis_max_air_temp_k': 41,\n",
    "    'reanalysis_min_air_temp_k': 40,\n",
    "    'reanalysis_precip_amt_kg_per_m2': 3,\n",
    "    'reanalysis_relative_humidity_percent': 7,\n",
    "    'reanalysis_sat_precip_amt_mm': 33,\n",
    "    'reanalysis_specific_humidity_g_per_kg': 26,\n",
    "    'reanalysis_tdtr_k': 34,\n",
    "    'station_avg_temp_c': 40,\n",
    "    'station_diur_temp_rng_c': 26,\n",
    "    'station_max_temp_c': 39,\n",
    "    'station_min_temp_c': 25,\n",
    "    'station_precip_mm':10,\n",
    "    'weekofyear': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reanalysis_air_temp_k',\n",
       " 'reanalysis_avg_temp_k',\n",
       " 'reanalysis_dew_point_temp_k',\n",
       " 'reanalysis_max_air_temp_k',\n",
       " 'reanalysis_min_air_temp_k',\n",
       " 'reanalysis_precip_amt_kg_per_m2',\n",
       " 'reanalysis_relative_humidity_percent',\n",
       " 'reanalysis_sat_precip_amt_mm',\n",
       " 'reanalysis_specific_humidity_g_per_kg',\n",
       " 'station_avg_temp_c',\n",
       " 'station_diur_temp_rng_c',\n",
       " 'station_max_temp_c',\n",
       " 'station_min_temp_c',\n",
       " 'weekofyear']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sj_datasets, sj_norm_scale, sj_columns = generate_lstm_data(\n",
    "    train_file, \n",
    "    single_step=True, \n",
    "    history_size=52, \n",
    "    cols=new_sj_cols, \n",
    "    norm_cols=new_sj_norm, \n",
    "    scale_cols=new_sj_scale,\n",
    "    extra_columns=extra_sj_cols,\n",
    "    prepend_with_file=train_file,\n",
    "    train_frac=1.0,\n",
    "    group_by_column=True\n",
    ")\n",
    "sj_train_x, sj_train_y = sj_datasets[0]\n",
    "sj_train_x = np.array(sj_train_x)\n",
    "sj_train_y = np.array(sj_train_y)\n",
    "iq_datasets, iq_norm_scale, iq_columns = generate_lstm_data(\n",
    "    train_file, \n",
    "    single_step=True, \n",
    "    history_size=52, \n",
    "    cols=new_iq_cols, \n",
    "    norm_cols=new_iq_norm, \n",
    "    scale_cols=new_iq_scale,\n",
    "    extra_columns=extra_iq_cols,\n",
    "    prepend_with_file=train_file,\n",
    "    train_frac=1.0,\n",
    "    group_by_column=True\n",
    ")\n",
    "iq_train_x, iq_train_y = iq_datasets[1]\n",
    "iq_train_x = np.array(iq_train_x)\n",
    "iq_train_y = np.array(iq_train_y)\n",
    "sj_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.28415113, -0.77349674, -0.31767771, -0.1531716 ,  0.27180253,\n",
       "        0.36091001,  0.02275855,  0.33006511,  0.31978348,  0.78017211,\n",
       "        0.26723291,  0.3906125 ,  0.30378983,  0.54712179,  0.96981111,\n",
       "        0.62366283,  0.92297256,  0.75161203,  1.08062425,  1.19600701,\n",
       "        0.54255217,  0.9983712 ,  1.27711767,  0.54255217,  0.01361932,\n",
       "       -0.22057341,  0.43859345,  0.63508686,  0.9366814 ,  0.26609051,\n",
       "       -0.75179107, -0.50845911, -0.78035116, -0.70495252, -0.91172757,\n",
       "       -1.69427401, -1.20304048, -1.4041035 , -1.03510715, -0.90715795,\n",
       "       -1.15048991, -1.83478965, -1.41095793, -1.3378441 , -1.12078742,\n",
       "       -1.39724908, -1.41666994, -1.36640419, -1.6908468 , -1.44408763,\n",
       "       -1.51491665, -0.88316748])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sj_train_x[53][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.36640419, -1.6908468 , -1.44408763, -1.51491665, -0.88316748])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sj_train_x[53][0][-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sj_train_y[53]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(936, 368)\n",
      "(520, 363)\n"
     ]
    }
   ],
   "source": [
    "trimed_sj_x = []\n",
    "trimed_iq_x = []\n",
    "\n",
    "def trim_data(data, columns, size_cols):\n",
    "    trimed_data = []\n",
    "    for row_i in range(data.shape[0]):\n",
    "        new_row = []\n",
    "        for col_i, col in enumerate(columns):\n",
    "            new_row = np.concatenate((new_row, data[row_i][col_i][-size_cols[col]:]), axis=None)\n",
    "\n",
    "        trimed_data.append(new_row)\n",
    "    return np.array(trimed_data)\n",
    "    \n",
    "trimed_sj_x = trim_data(sj_train_x, sj_columns, sj_col_size)\n",
    "trimed_iq_x = trim_data(iq_train_x, iq_columns, iq_col_size)\n",
    "\n",
    "print(trimed_sj_x.shape)\n",
    "print(trimed_iq_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "BUFFER_SIZE = 500\n",
    "train_sj_data_single = tf.data.Dataset.from_tensor_slices((trimed_sj_x, sj_train_y))\n",
    "train_sj_data_single = train_sj_data_single.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat(10)\n",
    "\n",
    "# val_sj_data_single = tf.data.Dataset.from_tensor_slices((sj_val_x, sj_val_y))\n",
    "# val_sj_data_single = val_sj_data_single.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build network with \n",
    "def build_model(optimizer = None, nodes=256, input_shape=trimed_sj_x.shape[-1]):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=input_shape),\n",
    "    tf.keras.layers.Dense(nodes, activation='selu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(nodes/2, activation='selu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  if not optimizer:\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.9999, amsgrad=False)\n",
    "\n",
    "  model.compile(loss='mae',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mae', 'mse'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 200 steps\n",
      "Epoch 1/80\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 30.6963 - mae: 30.5617 - mse: 3793.1257\n",
      "Epoch 2/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 22.0166 - mae: 22.0069 - mse: 1457.0677\n",
      "Epoch 3/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 20.6533 - mae: 20.7108 - mse: 1375.0470\n",
      "Epoch 4/80\n",
      "198/200 [============================>.] - ETA: 0s - loss: 20.2466 - mae: 20.3008 - mse: 1322.5087\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.007999999821186066.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 20.2074 - mae: 20.2605 - mse: 1316.8419\n",
      "Epoch 5/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 17.2485 - mae: 17.2871 - mse: 933.5552\n",
      "Epoch 6/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 17.1345 - mae: 17.1656 - mse: 931.2495\n",
      "Epoch 7/80\n",
      "195/200 [============================>.] - ETA: 0s - loss: 16.8316 - mae: 16.8242 - mse: 849.4490\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.006399999558925629.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 16.8634 - mae: 16.8565 - mse: 863.9751\n",
      "Epoch 8/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 15.4599 - mae: 15.4781 - mse: 714.5957\n",
      "Epoch 9/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 15.4196 - mae: 15.4110 - mse: 757.5063\n",
      "Epoch 10/80\n",
      "196/200 [============================>.] - ETA: 0s - loss: 14.6282 - mae: 14.6576 - mse: 642.3185\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0051199994981288915.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 14.6034 - mae: 14.6320 - mse: 637.9139\n",
      "Epoch 11/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 13.8051 - mae: 13.8384 - mse: 581.2571\n",
      "Epoch 12/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 13.7307 - mae: 13.7341 - mse: 591.5395\n",
      "Epoch 13/80\n",
      "194/200 [============================>.] - ETA: 0s - loss: 14.2754 - mae: 14.3118 - mse: 726.9434\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.004095999523997307.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 14.3308 - mae: 14.3779 - mse: 723.1644\n",
      "Epoch 14/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 12.8457 - mae: 12.8389 - mse: 531.0748\n",
      "Epoch 15/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 12.6189 - mae: 12.6301 - mse: 496.2288\n",
      "Epoch 16/80\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 12.4712 - mae: 12.4547 - mse: 498.5995\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0032767996191978457.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 12.4942 - mae: 12.4787 - mse: 492.7706\n",
      "Epoch 17/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 11.6192 - mae: 11.6280 - mse: 436.7689\n",
      "Epoch 18/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 11.3951 - mae: 11.4393 - mse: 427.9541\n",
      "Epoch 19/80\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 11.2673 - mae: 11.2872 - mse: 353.2836\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0026214396581053737.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 11.2245 - mae: 11.2432 - mse: 349.2353\n",
      "Epoch 20/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 10.5878 - mae: 10.5892 - mse: 338.9611\n",
      "Epoch 21/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 10.3508 - mae: 10.3544 - mse: 320.9257\n",
      "Epoch 22/80\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 10.7095 - mae: 10.6383 - mse: 334.6953\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0020971518009901048.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 10.6564 - mae: 10.5881 - mse: 335.4389\n",
      "Epoch 23/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 9.8366 - mae: 9.8315 - mse: 305.4288\n",
      "Epoch 24/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 9.5294 - mae: 9.5388 - mse: 269.5956\n",
      "Epoch 25/80\n",
      "191/200 [===========================>..] - ETA: 0s - loss: 9.3633 - mae: 9.3661 - mse: 282.5424\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0016777213662862779.\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 9.2710 - mae: 9.2729 - mse: 274.4457\n",
      "Epoch 26/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 9.1533 - mae: 9.1140 - mse: 237.4390\n",
      "Epoch 27/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 8.8388 - mae: 8.8427 - mse: 229.5432\n",
      "Epoch 28/80\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 8.5573 - mae: 8.5560 - mse: 229.7513\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0013421771116554739.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 8.5180 - mae: 8.5165 - mse: 224.9128\n",
      "Epoch 29/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 8.5834 - mae: 8.5991 - mse: 237.2930\n",
      "Epoch 30/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 8.3465 - mae: 8.3230 - mse: 204.7300\n",
      "Epoch 31/80\n",
      "198/200 [============================>.] - ETA: 0s - loss: 8.1723 - mae: 8.1830 - mse: 205.3659\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.001073741726577282.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 8.1636 - mae: 8.1741 - mse: 204.2290\n",
      "Epoch 32/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 8.0871 - mae: 8.0936 - mse: 188.7967\n",
      "Epoch 33/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 7.7458 - mae: 7.7632 - mse: 179.4741\n",
      "Epoch 34/80\n",
      "194/200 [============================>.] - ETA: 0s - loss: 7.7196 - mae: 7.7097 - mse: 170.3424\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0008589933626353742.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 7.7638 - mae: 7.7548 - mse: 172.6259\n",
      "Epoch 35/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 7.5142 - mae: 7.5257 - mse: 169.5383\n",
      "Epoch 36/80\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.6173 - mae: 7.6208 - mse: 180.2691\n",
      "Epoch 37/80\n",
      "194/200 [============================>.] - ETA: 0s - loss: 7.4821 - mae: 7.4933 - mse: 166.6495\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0006871947087347508.\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.4914 - mae: 7.5023 - mse: 166.1796\n",
      "Epoch 38/80\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 7.3017 - mae: 7.3221 - mse: 168.2110\n",
      "Epoch 39/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.9764 - mae: 6.9731 - mse: 132.8826\n",
      "Epoch 40/80\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 6.9944 - mae: 7.0080 - mse: 153.0894\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0005497557576745749.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.9824 - mae: 6.9953 - mse: 151.9408\n",
      "Epoch 41/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.9256 - mae: 6.9308 - mse: 143.4667\n",
      "Epoch 42/80\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 7.0021 - mae: 6.9997 - mse: 137.2614\n",
      "Epoch 43/80\n",
      "198/200 [============================>.] - ETA: 0s - loss: 6.7387 - mae: 6.7570 - mse: 130.6398\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.0004398046061396599.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.7347 - mae: 6.7528 - mse: 130.2899\n",
      "Epoch 44/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.7284 - mae: 6.7243 - mse: 137.5942\n",
      "Epoch 45/80\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 6.8834 - mae: 6.8491 - mse: 141.0981\n",
      "Epoch 46/80\n",
      "196/200 [============================>.] - ETA: 0s - loss: 6.7667 - mae: 6.7856 - mse: 157.3250\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.00035184368025511505.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.7984 - mae: 6.8171 - mse: 159.5323\n",
      "Epoch 47/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.7777 - mae: 6.7884 - mse: 144.1125\n",
      "Epoch 48/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.7171 - mae: 6.7353 - mse: 145.0457\n",
      "Epoch 49/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198/200 [============================>.] - ETA: 0s - loss: 6.7999 - mae: 6.7832 - mse: 153.0466\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.0002814749488607049.\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 6.8069 - mae: 6.7906 - mse: 154.2197\n",
      "Epoch 50/80\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 6.7376 - mae: 6.7449 - mse: 144.8644\n",
      "Epoch 51/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.6654 - mae: 6.6708 - mse: 151.7277\n",
      "Epoch 52/80\n",
      "197/200 [============================>.] - ETA: 0s - loss: 6.5854 - mae: 6.6048 - mse: 155.8985\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 0.0002251799684017897.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.6027 - mae: 6.6219 - mse: 158.1036\n",
      "Epoch 53/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.3259 - mae: 6.3071 - mse: 111.2628\n",
      "Epoch 54/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.0987 - mae: 6.1166 - mse: 113.7149\n",
      "Epoch 55/80\n",
      "196/200 [============================>.] - ETA: 0s - loss: 6.3373 - mae: 6.3427 - mse: 119.2009\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 0.0001801439793780446.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.3460 - mae: 6.3514 - mse: 123.0238\n",
      "Epoch 56/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.4564 - mae: 6.4636 - mse: 146.9793\n",
      "Epoch 57/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.3740 - mae: 6.3861 - mse: 132.1469\n",
      "Epoch 58/80\n",
      "194/200 [============================>.] - ETA: 0s - loss: 6.1881 - mae: 6.2000 - mse: 110.0579\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 0.00014411518350243568.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.2100 - mae: 6.2217 - mse: 115.0721\n",
      "Epoch 59/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.2446 - mae: 6.2586 - mse: 133.9338\n",
      "Epoch 60/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.2600 - mae: 6.2701 - mse: 118.4032\n",
      "Epoch 61/80\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 6.0854 - mae: 6.0893 - mse: 121.4715\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 0.00011529214680194855.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.0868 - mae: 6.0905 - mse: 120.3995\n",
      "Epoch 62/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.1747 - mae: 6.1898 - mse: 119.6934\n",
      "Epoch 63/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.4671 - mae: 6.4674 - mse: 140.2753\n",
      "Epoch 64/80\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 6.1979 - mae: 6.2234 - mse: 122.8150\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 9.223371744155885e-05.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.2050 - mae: 6.2294 - mse: 121.5340\n",
      "Epoch 65/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.1244 - mae: 6.1021 - mse: 121.8723\n",
      "Epoch 66/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.1350 - mae: 6.1230 - mse: 115.1247\n",
      "Epoch 67/80\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 6.2119 - mae: 6.2250 - mse: 124.3195\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 7.378697628155351e-05.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.2318 - mae: 6.2446 - mse: 123.0156\n",
      "Epoch 68/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.0496 - mae: 6.0589 - mse: 115.8299\n",
      "Epoch 69/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.3632 - mae: 6.3828 - mse: 130.7156\n",
      "Epoch 70/80\n",
      "194/200 [============================>.] - ETA: 0s - loss: 5.9843 - mae: 5.9884 - mse: 106.5421\n",
      "Epoch 00070: ReduceLROnPlateau reducing learning rate to 5.9029582189396024e-05.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 5.9649 - mae: 5.9687 - mse: 105.9203\n",
      "Epoch 71/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.1082 - mae: 6.1072 - mse: 118.5168\n",
      "Epoch 72/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 5.9669 - mae: 5.9617 - mse: 104.1388\n",
      "Epoch 73/80\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 6.0109 - mae: 6.0345 - mse: 103.5351\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 4.722366575151682e-05.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.0144 - mae: 6.0369 - mse: 104.1333\n",
      "Epoch 74/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 5.9839 - mae: 5.9775 - mse: 103.7945\n",
      "Epoch 75/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.2246 - mae: 6.2464 - mse: 126.2634\n",
      "Epoch 76/80\n",
      "196/200 [============================>.] - ETA: 0s - loss: 6.1282 - mae: 6.1115 - mse: 125.2192\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 3.777893143706024e-05.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.1213 - mae: 6.1049 - mse: 123.8668\n",
      "Epoch 77/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.1680 - mae: 6.1852 - mse: 119.3775\n",
      "Epoch 78/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.1043 - mae: 6.1134 - mse: 114.2976\n",
      "Epoch 79/80\n",
      "191/200 [===========================>..] - ETA: 0s - loss: 6.0433 - mae: 6.0424 - mse: 117.1516\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 3.022314631380141e-05.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.0336 - mae: 6.0326 - mse: 115.8276\n",
      "Epoch 80/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 5.9981 - mae: 6.0001 - mse: 106.4875\n"
     ]
    }
   ],
   "source": [
    "log_dir=\"logs/fit/lstm\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# create model\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=0.005, momentum=0.01, nesterov=False, name='SGD')\n",
    "\n",
    "\n",
    "# Train the Model.\n",
    "EVALUATION_INTERVAL = 200\n",
    "EPOCHS = 80\n",
    "\n",
    "# opt = tf.keras.optimizers.Adam(learning_rate=0.1, beta_1=0.9, beta_2=0.9999, amsgrad=False)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"mae\", factor=0.8, patience=3, min_lr=1e-6, verbose=1,\n",
    "                                                     mode=\"max\")\n",
    "\n",
    "train_sj_data_single = tf.data.Dataset.from_tensor_slices((trimed_sj_x, sj_train_y))\n",
    "train_sj_data_single = train_sj_data_single.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "opt = tf.keras.optimizers.RMSprop(\n",
    "    learning_rate=0.01, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False,\n",
    "    name='RMSprop'\n",
    ")\n",
    "sj_model = build_model(optimizer=opt, nodes=800)\n",
    "history = sj_model.fit(\n",
    "    train_sj_data_single,\n",
    "    epochs=EPOCHS, \n",
    "    steps_per_epoch=EVALUATION_INTERVAL,\n",
    "    verbose=1,\n",
    "    callbacks=[tensorboard_callback, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 200 steps\n",
      "Epoch 1/80\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 7.5553 - mae: 7.5943 - mse: 147.3157\n",
      "Epoch 2/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.2995 - mae: 6.3470 - mse: 117.4251\n",
      "Epoch 3/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.1737 - mae: 6.2202 - mse: 119.6572\n",
      "Epoch 4/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.0651 - mae: 6.1167 - mse: 120.1595\n",
      "Epoch 5/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 5.8449 - mae: 5.8898 - mse: 116.3052\n",
      "Epoch 6/80\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 5.7507 - mae: 5.7992 - mse: 114.8890\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 5.7673 - mae: 5.8141 - mse: 113.7187\n",
      "Epoch 7/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 5.6411 - mae: 5.6900 - mse: 111.0401\n",
      "Epoch 8/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 5.5581 - mae: 5.6085 - mse: 114.3731\n",
      "Epoch 9/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 5.3045 - mae: 5.3543 - mse: 102.9779\n",
      "Epoch 10/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 5.2015 - mae: 5.2407 - mse: 98.3107\n",
      "Epoch 11/80\n",
      "198/200 [============================>.] - ETA: 0s - loss: 4.9221 - mae: 4.9650 - mse: 89.0239\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 4.9192 - mae: 4.9614 - mse: 88.4923\n",
      "Epoch 12/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 4.6934 - mae: 4.7342 - mse: 81.1399\n",
      "Epoch 13/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 4.6826 - mae: 4.7181 - mse: 78.8587\n",
      "Epoch 14/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 4.4380 - mae: 4.4760 - mse: 72.7252\n",
      "Epoch 15/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 4.3494 - mae: 4.3840 - mse: 67.7648\n",
      "Epoch 16/80\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 4.2399 - mae: 4.2744 - mse: 65.6386\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 4.2049 - mae: 4.2375 - mse: 64.4529\n",
      "Epoch 17/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 4.0157 - mae: 4.0594 - mse: 60.3777\n",
      "Epoch 18/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 3.9621 - mae: 3.9974 - mse: 57.9486\n",
      "Epoch 19/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 3.9180 - mae: 3.9570 - mse: 58.6086\n",
      "Epoch 20/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 3.7791 - mae: 3.8090 - mse: 53.6750\n",
      "Epoch 21/80\n",
      "194/200 [============================>.] - ETA: 0s - loss: 3.7260 - mae: 3.7554 - mse: 52.3401\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 3.7634 - mae: 3.7924 - mse: 52.4921\n",
      "Epoch 22/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 3.5887 - mae: 3.6145 - mse: 49.1278\n",
      "Epoch 23/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 3.5505 - mae: 3.5768 - mse: 48.9005\n",
      "Epoch 24/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 3.5481 - mae: 3.5767 - mse: 50.9955\n",
      "Epoch 25/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 3.3442 - mae: 3.3734 - mse: 43.9413\n",
      "Epoch 26/80\n",
      "191/200 [===========================>..] - ETA: 0s - loss: 3.2144 - mae: 3.2357 - mse: 39.7341\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.00032768002711236477.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 3.2378 - mae: 3.2583 - mse: 41.0005\n",
      "Epoch 27/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 3.1417 - mae: 3.1696 - mse: 38.9629\n",
      "Epoch 28/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 3.1004 - mae: 3.1217 - mse: 38.7805\n",
      "Epoch 29/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 3.1166 - mae: 3.1367 - mse: 39.5480\n",
      "Epoch 30/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.9792 - mae: 2.9990 - mse: 36.4575\n",
      "Epoch 31/80\n",
      "198/200 [============================>.] - ETA: 0s - loss: 2.9663 - mae: 2.9882 - mse: 35.6581\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0002621440216898918.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.9768 - mae: 2.9986 - mse: 35.5954\n",
      "Epoch 32/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.8282 - mae: 2.8479 - mse: 33.2849\n",
      "Epoch 33/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.7881 - mae: 2.8130 - mse: 32.5899\n",
      "Epoch 34/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.8044 - mae: 2.8242 - mse: 32.1546\n",
      "Epoch 35/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.7954 - mae: 2.8186 - mse: 32.4232\n",
      "Epoch 36/80\n",
      "197/200 [============================>.] - ETA: 0s - loss: 2.7004 - mae: 2.7141 - mse: 30.2227\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.00020971521735191345.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.6893 - mae: 2.7026 - mse: 29.9169\n",
      "Epoch 37/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.6727 - mae: 2.6865 - mse: 28.2696\n",
      "Epoch 38/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.5350 - mae: 2.5544 - mse: 27.1024\n",
      "Epoch 39/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.6058 - mae: 2.6246 - mse: 27.9395\n",
      "Epoch 40/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.5630 - mae: 2.5768 - mse: 27.1621\n",
      "Epoch 41/80\n",
      "195/200 [============================>.] - ETA: 0s - loss: 2.4790 - mae: 2.4965 - mse: 26.1415\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.00016777217388153076.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.5227 - mae: 2.5404 - mse: 28.4573\n",
      "Epoch 42/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.4863 - mae: 2.5008 - mse: 25.7108\n",
      "Epoch 43/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.4596 - mae: 2.4793 - mse: 26.4962\n",
      "Epoch 44/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3595 - mae: 2.3793 - mse: 23.9624\n",
      "Epoch 45/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3653 - mae: 2.3854 - mse: 22.7079\n",
      "Epoch 46/80\n",
      "194/200 [============================>.] - ETA: 0s - loss: 2.4039 - mae: 2.4255 - mse: 25.2211\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.00013421773910522462.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.4072 - mae: 2.4281 - mse: 25.0763\n",
      "Epoch 47/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3147 - mae: 2.3313 - mse: 20.4334\n",
      "Epoch 48/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.2861 - mae: 2.3060 - mse: 20.6984\n",
      "Epoch 49/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3191 - mae: 2.3341 - mse: 23.0922\n",
      "Epoch 50/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.3118 - mae: 2.3316 - mse: 24.4883\n",
      "Epoch 51/80\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 2.3112 - mae: 2.3296 - mse: 21.2477\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.00010737419361248613.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.2684 - mae: 2.2883 - mse: 20.5732\n",
      "Epoch 52/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.2116 - mae: 2.2252 - mse: 21.1131\n",
      "Epoch 53/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.2402 - mae: 2.2576 - mse: 20.6660\n",
      "Epoch 54/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.2391 - mae: 2.2545 - mse: 20.7371\n",
      "Epoch 55/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.1902 - mae: 2.2060 - mse: 21.4719\n",
      "Epoch 56/80\n",
      "199/200 [============================>.] - ETA: 0s - loss: 2.2114 - mae: 2.2293 - mse: 21.8884\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 8.589935605414213e-05.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.2054 - mae: 2.2230 - mse: 21.7856\n",
      "Epoch 57/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 1s 6ms/step - loss: 2.1859 - mae: 2.2043 - mse: 20.0088\n",
      "Epoch 58/80\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 2.1694 - mae: 2.1842 - mse: 18.9826\n",
      "Epoch 59/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.1342 - mae: 2.1523 - mse: 18.8316\n",
      "Epoch 60/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.1485 - mae: 2.1587 - mse: 20.3685\n",
      "Epoch 61/80\n",
      "198/200 [============================>.] - ETA: 0s - loss: 2.1872 - mae: 2.2058 - mse: 18.9660\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 6.871948717162013e-05.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.1792 - mae: 2.1974 - mse: 18.8038\n",
      "Epoch 62/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.0807 - mae: 2.0976 - mse: 17.7004\n",
      "Epoch 63/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.1081 - mae: 2.1238 - mse: 18.1654\n",
      "Epoch 64/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.0999 - mae: 2.1103 - mse: 17.4883\n",
      "Epoch 65/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.0599 - mae: 2.0734 - mse: 16.3813\n",
      "Epoch 66/80\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 2.0951 - mae: 2.1091 - mse: 18.4431\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 5.497558740898967e-05.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.0826 - mae: 2.0982 - mse: 18.0708\n",
      "Epoch 67/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.0882 - mae: 2.1050 - mse: 19.5249\n",
      "Epoch 68/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.0032 - mae: 2.0172 - mse: 16.4041\n",
      "Epoch 69/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.0111 - mae: 2.0268 - mse: 16.4216\n",
      "Epoch 70/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.1099 - mae: 2.1262 - mse: 18.3950\n",
      "Epoch 71/80\n",
      "190/200 [===========================>..] - ETA: 0s - loss: 2.0184 - mae: 2.0307 - mse: 16.5381\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 4.398046876303852e-05.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.0205 - mae: 2.0322 - mse: 16.3554\n",
      "Epoch 72/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.0122 - mae: 2.0231 - mse: 17.9347\n",
      "Epoch 73/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 1.9922 - mae: 2.0068 - mse: 16.9152\n",
      "Epoch 74/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 2.0283 - mae: 2.0456 - mse: 16.4262\n",
      "Epoch 75/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 1.9983 - mae: 2.0123 - mse: 18.3120\n",
      "Epoch 76/80\n",
      "196/200 [============================>.] - ETA: 0s - loss: 1.9984 - mae: 2.0114 - mse: 15.2213\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 3.518437442835421e-05.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 1.9902 - mae: 2.0027 - mse: 15.0240\n",
      "Epoch 77/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 1.9760 - mae: 1.9904 - mse: 17.7662\n",
      "Epoch 78/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 1.9716 - mae: 1.9857 - mse: 14.8272\n",
      "Epoch 79/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 1.9922 - mae: 2.0070 - mse: 16.3823\n",
      "Epoch 80/80\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 1.9608 - mae: 1.9743 - mse: 15.3892\n"
     ]
    }
   ],
   "source": [
    "train_iq_data_single = tf.data.Dataset.from_tensor_slices((trimed_iq_x, iq_train_y))\n",
    "train_iq_data_single = train_iq_data_single.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "EPOCHS = 80\n",
    "# opt = tf.keras.optimizers.Adam(learning_rate=0.1, beta_1=0.9, beta_2=0.9999, amsgrad=False)\n",
    "opt = tf.keras.optimizers.RMSprop(\n",
    "    learning_rate=0.001, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False,\n",
    "    name='RMSprop'\n",
    ")\n",
    "iq_model = build_model(optimizer=opt, nodes=400, input_shape=trimed_iq_x.shape[-1])\n",
    "train_iq_data_single = tf.data.Dataset.from_tensor_slices((trimed_iq_x, iq_train_y))\n",
    "train_iq_data_single = train_iq_data_single.cache().batch(BATCH_SIZE).repeat()\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"mae\", factor=0.8, patience=5, min_lr=1e-6, verbose=1,\n",
    "                                                     mode=\"max\")\n",
    "history = iq_model.fit(\n",
    "    train_iq_data_single,\n",
    "    epochs=EPOCHS, \n",
    "    steps_per_epoch=EVALUATION_INTERVAL,\n",
    "    verbose=1,\n",
    "    callbacks=[tensorboard_callback, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(260, 368)\n",
      "260\n"
     ]
    }
   ],
   "source": [
    "file = train_file\n",
    "file = test_file\n",
    "\n",
    "org_sj_test_data, test_scale, sj_columns = generate_lstm_data(\n",
    "    file, history_size=52, \n",
    "    cols=new_sj_cols_no_label, \n",
    "    norm_cols=new_sj_norm, \n",
    "    scale_cols=new_sj_scale, \n",
    "    single_step=True, \n",
    "    train_frac=1.0, train_scale=sj_norm_scale, \n",
    "    prepend_with_file=train_file,\n",
    "    extra_columns=extra_sj_cols,\n",
    "    group_by_column=True\n",
    ")\n",
    "org_iq_test_data, test_scale, iq_columns = generate_lstm_data(\n",
    "    file, history_size=52, \n",
    "    cols=new_iq_cols_no_label, \n",
    "    norm_cols=new_iq_norm, \n",
    "    scale_cols=new_iq_scale, \n",
    "    single_step=True, \n",
    "    train_frac=1.0, train_scale=iq_norm_scale, \n",
    "    prepend_with_file=train_file,\n",
    "    extra_columns=extra_iq_cols,\n",
    "    group_by_column=True\n",
    ")\n",
    "sj_test_x, sj_test_y = org_sj_test_data[0]\n",
    "sj_test_x = np.array(sj_test_x)\n",
    "sj_test_y = np.array(sj_test_y)\n",
    "iq_test_x, iq_test_y = org_iq_test_data[1]\n",
    "iq_test_x = np.array(iq_test_x)\n",
    "iq_test_y = np.array(iq_test_y)\n",
    "trimed_test_sj_x = trim_data(sj_test_x, sj_columns, sj_col_size)\n",
    "trimed_test_iq_x = trim_data(iq_test_x, iq_columns, iq_col_size)\n",
    "sj_test_set = tf.data.Dataset.from_tensor_slices((trimed_test_sj_x, sj_test_y)).batch(len(sj_test_y))\n",
    "print(trimed_test_sj_x.shape)\n",
    "\n",
    "sj_pred = []\n",
    "for x, y in sj_test_set.take(1):\n",
    "    predictions = sj_model.predict(x)\n",
    "    sj_pred = predictions.flatten()\n",
    "    print(len(predictions.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156\n"
     ]
    }
   ],
   "source": [
    "iq_test_set = tf.data.Dataset.from_tensor_slices((trimed_test_iq_x, iq_test_y)).batch(len(iq_test_y))\n",
    "iq_pred = []\n",
    "for x, y in iq_test_set.take(1):\n",
    "    predictions = iq_model.predict(x)\n",
    "    iq_pred = predictions.flatten()\n",
    "    print(len(predictions.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416\n",
      "asas\n"
     ]
    }
   ],
   "source": [
    "from preprocessing_helpers import export_test_to_csv\n",
    "preds = np.concatenate((sj_pred, iq_pred), axis=None)\n",
    "export_test_to_csv(predictions=preds,path=file, prefix='test' if file == test_file else 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Current Ranking (baseline)\n",
    "Image(\"img/current-27-05.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard\n",
    "# https://tensorboard.dev/experiment/rsdMubj0S165iOdLmbd13A/#scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sj_model.save('models/sj_model_17.57MAP.h5') \n",
    "# iq_model.save('models/iq_model_17.57MAP.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
