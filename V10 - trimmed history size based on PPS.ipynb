{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import functools\n",
    "from IPython.display import Image, clear_output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from data_info import *\n",
    "from preprocessing_helpers import *\n",
    "from datetime import datetime\n",
    "%matplotlib inline\n",
    "pd.set_option(\"display.precision\", 2)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_iq_norm = [\n",
    "                'precipitation_amt_mm',\n",
    "                'reanalysis_air_temp_k',\n",
    "                'reanalysis_avg_temp_k',\n",
    "                'reanalysis_dew_point_temp_k',\n",
    "                'reanalysis_max_air_temp_k',\n",
    "                'reanalysis_min_air_temp_k',\n",
    "                'reanalysis_precip_amt_kg_per_m2',\n",
    "                'reanalysis_relative_humidity_percent',\n",
    "                'reanalysis_sat_precip_amt_mm',\n",
    "                'reanalysis_specific_humidity_g_per_kg',\n",
    "                'reanalysis_tdtr_k',\n",
    "                'station_avg_temp_c',\n",
    "                'station_diur_temp_rng_c',\n",
    "                'station_max_temp_c',\n",
    "                'station_min_temp_c',\n",
    "                'station_precip_mm',\n",
    "]\n",
    "new_iq_scale = [\n",
    "                 'weekofyear',\n",
    "]\n",
    "\n",
    "extra_iq_cols = [\n",
    "]\n",
    "new_sj_norm = [\n",
    "                'precipitation_amt_mm',\n",
    "                'reanalysis_air_temp_k',\n",
    "                'reanalysis_avg_temp_k',\n",
    "                'reanalysis_dew_point_temp_k',\n",
    "                'reanalysis_max_air_temp_k',\n",
    "                'reanalysis_min_air_temp_k',\n",
    "                'reanalysis_precip_amt_kg_per_m2',\n",
    "                'reanalysis_relative_humidity_percent',\n",
    "                'reanalysis_sat_precip_amt_mm',\n",
    "                'reanalysis_specific_humidity_g_per_kg',\n",
    "                'reanalysis_tdtr_k',\n",
    "                'station_avg_temp_c',\n",
    "                'station_diur_temp_rng_c',\n",
    "                'station_max_temp_c',\n",
    "                'station_min_temp_c',\n",
    "                'station_precip_mm',\n",
    "]\n",
    "new_sj_scale = [\n",
    "                 'weekofyear',\n",
    "]\n",
    "\n",
    "extra_sj_cols = [\n",
    "]\n",
    "new_iq_cols = [LABEL_COLUMN] + CATEGORICAL_COLUMNS + new_iq_norm + new_iq_scale + extra_iq_cols + [DATETIME_COLUMN]\n",
    "new_iq_cols_no_label = CATEGORICAL_COLUMNS + new_iq_norm + new_iq_scale + extra_iq_cols + [DATETIME_COLUMN]\n",
    "new_sj_cols = [LABEL_COLUMN] + CATEGORICAL_COLUMNS + new_sj_norm + new_sj_scale + extra_sj_cols + [DATETIME_COLUMN]\n",
    "new_sj_cols_no_label = CATEGORICAL_COLUMNS + new_sj_norm + new_sj_scale + extra_sj_cols + [DATETIME_COLUMN]\n",
    "\n",
    "\n",
    "sj_col_size = {\n",
    "    'precipitation_amt_mm': 40,\n",
    "    'reanalysis_air_temp_k': 16,\n",
    "    'reanalysis_avg_temp_k': 15,\n",
    "    'reanalysis_dew_point_temp_k': 39,\n",
    "    'reanalysis_max_air_temp_k': 12,\n",
    "    'reanalysis_min_air_temp_k': 21,\n",
    "    'reanalysis_precip_amt_kg_per_m2': 30,\n",
    "    'reanalysis_relative_humidity_percent': 34,\n",
    "    'reanalysis_sat_precip_amt_mm': 40,\n",
    "    'reanalysis_specific_humidity_g_per_kg': 14,\n",
    "    'reanalysis_tdtr_k': 21,\n",
    "    'station_avg_temp_c': 41,\n",
    "    'station_diur_temp_rng_c': 40,\n",
    "    'station_max_temp_c': 37,\n",
    "    'station_min_temp_c': 26,\n",
    "    'station_precip_mm': 32,\n",
    "    'weekofyear': 3\n",
    "}\n",
    "iq_col_size = {\n",
    "    'precipitation_amt_mm': 33,\n",
    "    'reanalysis_air_temp_k': 10,\n",
    "    'reanalysis_avg_temp_k': 4,\n",
    "    'reanalysis_dew_point_temp_k': 6,\n",
    "    'reanalysis_max_air_temp_k': 41,\n",
    "    'reanalysis_min_air_temp_k': 40,\n",
    "    'reanalysis_precip_amt_kg_per_m2': 3,\n",
    "    'reanalysis_relative_humidity_percent': 7,\n",
    "    'reanalysis_sat_precip_amt_mm': 33,\n",
    "    'reanalysis_specific_humidity_g_per_kg': 26,\n",
    "    'reanalysis_tdtr_k': 34,\n",
    "    'station_avg_temp_c': 40,\n",
    "    'station_diur_temp_rng_c': 26,\n",
    "    'station_max_temp_c': 39,\n",
    "    'station_min_temp_c': 25,\n",
    "    'station_precip_mm':10,\n",
    "    'weekofyear': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['precipitation_amt_mm',\n",
       " 'reanalysis_air_temp_k',\n",
       " 'reanalysis_avg_temp_k',\n",
       " 'reanalysis_dew_point_temp_k',\n",
       " 'reanalysis_max_air_temp_k',\n",
       " 'reanalysis_min_air_temp_k',\n",
       " 'reanalysis_precip_amt_kg_per_m2',\n",
       " 'reanalysis_relative_humidity_percent',\n",
       " 'reanalysis_sat_precip_amt_mm',\n",
       " 'reanalysis_specific_humidity_g_per_kg',\n",
       " 'reanalysis_tdtr_k',\n",
       " 'station_avg_temp_c',\n",
       " 'station_diur_temp_rng_c',\n",
       " 'station_max_temp_c',\n",
       " 'station_min_temp_c',\n",
       " 'station_precip_mm',\n",
       " 'weekofyear']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sj_datasets, sj_norm_scale, sj_columns = generate_lstm_data(\n",
    "    train_file, \n",
    "    single_step=True, \n",
    "    history_size=52, \n",
    "    cols=new_sj_cols, \n",
    "    norm_cols=new_sj_norm, \n",
    "    scale_cols=new_sj_scale,\n",
    "    extra_columns=extra_sj_cols,\n",
    "    prepend_with_file=train_file,\n",
    "    train_frac=1.0,\n",
    "    group_by_column=True\n",
    ")\n",
    "sj_train_x, sj_train_y = sj_datasets[0]\n",
    "sj_train_x = np.array(sj_train_x)\n",
    "sj_train_y = np.array(sj_train_y)\n",
    "iq_datasets, iq_norm_scale, iq_columns = generate_lstm_data(\n",
    "    train_file, \n",
    "    single_step=True, \n",
    "    history_size=52, \n",
    "    cols=new_iq_cols, \n",
    "    norm_cols=new_iq_norm, \n",
    "    scale_cols=new_iq_scale,\n",
    "    extra_columns=extra_iq_cols,\n",
    "    prepend_with_file=train_file,\n",
    "    train_frac=1.0,\n",
    "    group_by_column=True\n",
    ")\n",
    "iq_train_x, iq_train_y = iq_datasets[1]\n",
    "iq_train_x = np.array(iq_train_x)\n",
    "iq_train_y = np.array(iq_train_y)\n",
    "sj_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5192724 , -0.28312527, -0.01700562, -0.45251543, -0.63053403,\n",
       "       -0.58375874, -0.72226811,  2.63011291, -0.36259787, -0.47408656,\n",
       "       -0.29561382,  0.54225437, -0.4270842 , -0.05969375, -0.14734067,\n",
       "        1.25932422, -0.06559743,  0.12831569, -0.14597828,  0.66464216,\n",
       "        2.46231221,  0.36559815, -0.09329931,  0.36627934,  2.45822504,\n",
       "        2.08311441,  1.50001264, -0.33671251, -0.34715748, -0.34624922,\n",
       "       -0.03812262, -0.57513028, -0.80128658,  0.38966699, -0.80128658,\n",
       "       -0.80128658, -0.44138926, -0.80128658, -0.80128658, -0.80128658,\n",
       "       -0.80128658, -0.39143506, -0.80128658, -0.07944837, -0.80128658,\n",
       "       -0.80128658, -0.80128658, -0.80128658, -0.50791918, -0.66504785,\n",
       "        0.21074012, -0.05833137])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sj_train_x[53][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.80128658, -0.50791918, -0.66504785,  0.21074012, -0.05833137])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sj_train_x[53][0][-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sj_train_y[53]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(936, 461)\n",
      "(520, 380)\n"
     ]
    }
   ],
   "source": [
    "trimed_sj_x = []\n",
    "trimed_iq_x = []\n",
    "\n",
    "def trim_data(data, columns, size_cols):\n",
    "    trimed_data = []\n",
    "    for row_i in range(data.shape[0]):\n",
    "        new_row = []\n",
    "        for col_i, col in enumerate(columns):\n",
    "            new_row = np.concatenate((new_row, data[row_i][col_i][-size_cols[col]:]), axis=None)\n",
    "\n",
    "        trimed_data.append(new_row)\n",
    "    return np.array(trimed_data)\n",
    "    \n",
    "trimed_sj_x = trim_data(sj_train_x, sj_columns, sj_col_size)\n",
    "trimed_iq_x = trim_data(iq_train_x, iq_columns, iq_col_size)\n",
    "\n",
    "print(trimed_sj_x.shape)\n",
    "print(trimed_iq_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "BUFFER_SIZE = 500\n",
    "train_sj_data_single = tf.data.Dataset.from_tensor_slices((trimed_sj_x, sj_train_y))\n",
    "train_sj_data_single = train_sj_data_single.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat(10)\n",
    "\n",
    "# val_sj_data_single = tf.data.Dataset.from_tensor_slices((sj_val_x, sj_val_y))\n",
    "# val_sj_data_single = val_sj_data_single.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build network with \n",
    "def build_model(optimizer = None, nodes=256, input_shape=trimed_sj_x.shape[-1]):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=input_shape),\n",
    "    tf.keras.layers.Dense(nodes, activation='selu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(nodes/2, activation='selu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  if not optimizer:\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.9999, amsgrad=False)\n",
    "\n",
    "  model.compile(loss='mae',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mae', 'mse'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 200 steps\n",
      "Epoch 1/40\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 23.0796 - mae: 23.0962 - mse: 2437.4932\n",
      "Epoch 2/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 19.9561 - mae: 20.0246 - mse: 1963.2388\n",
      "Epoch 3/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 18.4496 - mae: 18.4331 - mse: 1567.2140\n",
      "Epoch 4/40\n",
      "198/200 [============================>.] - ETA: 0s - loss: 17.1958 - mae: 17.1697 - mse: 1281.4452\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.007999999821186066.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 17.1549 - mae: 17.1289 - mse: 1273.9495\n",
      "Epoch 5/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 15.1156 - mae: 15.1400 - mse: 931.2686\n",
      "Epoch 6/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 15.2556 - mae: 15.2803 - mse: 954.6669\n",
      "Epoch 7/40\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 14.4299 - mae: 14.4132 - mse: 746.9147\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.006399999558925629.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 14.2946 - mae: 14.2777 - mse: 728.3293\n",
      "Epoch 8/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 13.7657 - mae: 13.7561 - mse: 748.6982\n",
      "Epoch 9/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 13.4318 - mae: 13.3939 - mse: 699.0139\n",
      "Epoch 10/40\n",
      "198/200 [============================>.] - ETA: 0s - loss: 12.7243 - mae: 12.6846 - mse: 600.8837\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0051199994981288915.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 12.6636 - mae: 12.6240 - mse: 596.0732\n",
      "Epoch 11/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 12.4802 - mae: 12.4877 - mse: 586.4214\n",
      "Epoch 12/40\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 11.7181 - mae: 11.7704 - mse: 545.1646\n",
      "Epoch 13/40\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 12.1168 - mae: 12.0523 - mse: 596.5416\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.004095999523997307.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 12.1788 - mae: 12.1194 - mse: 597.9995\n",
      "Epoch 14/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 11.9356 - mae: 11.9338 - mse: 556.5134\n",
      "Epoch 15/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 11.4911 - mae: 11.4460 - mse: 546.1495\n",
      "Epoch 16/40\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 11.2181 - mae: 11.2264 - mse: 452.0096\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0032767996191978457.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 11.3193 - mae: 11.3282 - mse: 463.1378\n",
      "Epoch 17/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 11.1284 - mae: 11.1410 - mse: 523.8655\n",
      "Epoch 18/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 10.8570 - mae: 10.8588 - mse: 443.3708\n",
      "Epoch 19/40\n",
      "196/200 [============================>.] - ETA: 0s - loss: 10.7019 - mae: 10.7303 - mse: 435.7356\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0026214396581053737.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 10.6806 - mae: 10.7082 - mse: 432.1521\n",
      "Epoch 20/40\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.5516 - mae: 10.5442 - mse: 452.8057\n",
      "Epoch 21/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 10.4532 - mae: 10.4615 - mse: 456.1421\n",
      "Epoch 22/40\n",
      "196/200 [============================>.] - ETA: 0s - loss: 10.5723 - mae: 10.5567 - mse: 440.6039\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0020971518009901048.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 10.5296 - mae: 10.5141 - mse: 437.0139\n",
      "Epoch 23/40\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 10.1406 - mae: 10.1672 - mse: 404.1184\n",
      "Epoch 24/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 10.0430 - mae: 10.0893 - mse: 400.1507\n",
      "Epoch 25/40\n",
      "191/200 [===========================>..] - ETA: 0s - loss: 10.2718 - mae: 10.2570 - mse: 419.4651\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0016777213662862779.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 10.1850 - mae: 10.1702 - mse: 407.7476\n",
      "Epoch 26/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 10.0564 - mae: 9.9765 - mse: 388.8766\n",
      "Epoch 27/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 10.1551 - mae: 10.1867 - mse: 466.4833\n",
      "Epoch 28/40\n",
      "195/200 [============================>.] - ETA: 0s - loss: 9.5718 - mae: 9.5752 - mse: 385.2050\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0013421771116554739.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 9.6473 - mae: 9.6511 - mse: 401.7608\n",
      "Epoch 29/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 9.4825 - mae: 9.4882 - mse: 359.8116\n",
      "Epoch 30/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 9.8560 - mae: 9.8629 - mse: 414.0620\n",
      "Epoch 31/40\n",
      "192/200 [===========================>..] - ETA: 0s - loss: 9.5097 - mae: 9.4963 - mse: 351.9154\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.001073741726577282.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 9.6028 - mae: 9.5857 - mse: 358.0365\n",
      "Epoch 32/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 9.5261 - mae: 9.5394 - mse: 360.8460\n",
      "Epoch 33/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 9.3433 - mae: 9.3537 - mse: 355.4442\n",
      "Epoch 34/40\n",
      "198/200 [============================>.] - ETA: 0s - loss: 9.0560 - mae: 9.0794 - mse: 291.0643\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0008589933626353742.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 9.0625 - mae: 9.0857 - mse: 291.4179\n",
      "Epoch 35/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 9.2846 - mae: 9.3151 - mse: 366.6864\n",
      "Epoch 36/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 9.0592 - mae: 9.0136 - mse: 317.7770\n",
      "Epoch 37/40\n",
      "196/200 [============================>.] - ETA: 0s - loss: 9.3998 - mae: 9.4066 - mse: 339.2166\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0006871947087347508.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 9.3916 - mae: 9.3982 - mse: 337.5530\n",
      "Epoch 38/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 9.0225 - mae: 9.0304 - mse: 339.3083\n",
      "Epoch 39/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 9.1380 - mae: 9.1177 - mse: 355.2652\n",
      "Epoch 40/40\n",
      "197/200 [============================>.] - ETA: 0s - loss: 9.2193 - mae: 9.2414 - mse: 349.1580\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0005497557576745749.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 9.2233 - mae: 9.2449 - mse: 353.2628\n"
     ]
    }
   ],
   "source": [
    "log_dir=\"logs/fit/lstm\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# create model\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=0.005, momentum=0.01, nesterov=False, name='SGD')\n",
    "\n",
    "\n",
    "# Train the Model.\n",
    "EVALUATION_INTERVAL = 200\n",
    "EPOCHS = 40\n",
    "\n",
    "# opt = tf.keras.optimizers.Adam(learning_rate=0.1, beta_1=0.9, beta_2=0.9999, amsgrad=False)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"mae\", factor=0.8, patience=3, min_lr=1e-6, verbose=1,\n",
    "                                                     mode=\"max\")\n",
    "\n",
    "train_sj_data_single = tf.data.Dataset.from_tensor_slices((trimed_sj_x, sj_train_y))\n",
    "train_sj_data_single = train_sj_data_single.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "opt = tf.keras.optimizers.RMSprop(\n",
    "    learning_rate=0.01, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False,\n",
    "    name='RMSprop'\n",
    ")\n",
    "sj_model = build_model(optimizer=opt, nodes=100)\n",
    "history = sj_model.fit(\n",
    "    train_sj_data_single,\n",
    "    epochs=EPOCHS, \n",
    "    steps_per_epoch=EVALUATION_INTERVAL,\n",
    "    verbose=1,\n",
    "    callbacks=[tensorboard_callback, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 200 steps\n",
      "Epoch 1/40\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 7.0197 - mae: 7.0598 - mse: 138.9400\n",
      "Epoch 2/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.4552 - mae: 6.5064 - mse: 126.5591\n",
      "Epoch 3/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.1395 - mae: 6.1846 - mse: 121.5201\n",
      "Epoch 4/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 6.0758 - mae: 6.1327 - mse: 122.3543\n",
      "Epoch 5/40\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 5.7893 - mae: 5.8385 - mse: 114.8500\n",
      "Epoch 6/40\n",
      "198/200 [============================>.] - ETA: 0s - loss: 5.7582 - mae: 5.8070 - mse: 112.8525\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 5.7193 - mae: 5.7668 - mse: 111.7747\n",
      "Epoch 7/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 5.6489 - mae: 5.7063 - mse: 112.8428\n",
      "Epoch 8/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 5.5810 - mae: 5.6326 - mse: 116.7474\n",
      "Epoch 9/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 5.4482 - mae: 5.5017 - mse: 108.9391\n",
      "Epoch 10/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 5.3107 - mae: 5.3595 - mse: 105.0120\n",
      "Epoch 11/40\n",
      "191/200 [===========================>..] - ETA: 0s - loss: 5.2606 - mae: 5.3081 - mse: 100.4519\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 5.2990 - mae: 5.3447 - mse: 104.1864\n",
      "Epoch 12/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 5.1628 - mae: 5.2085 - mse: 102.1838\n",
      "Epoch 13/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 5.1145 - mae: 5.1611 - mse: 102.1226\n",
      "Epoch 14/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 5.0468 - mae: 5.0895 - mse: 97.4258\n",
      "Epoch 15/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 4.8767 - mae: 4.9240 - mse: 93.7252\n",
      "Epoch 16/40\n",
      "198/200 [============================>.] - ETA: 0s - loss: 4.9192 - mae: 4.9677 - mse: 95.3247\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 4.9034 - mae: 4.9509 - mse: 94.5578\n",
      "Epoch 17/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 4.6324 - mae: 4.6877 - mse: 88.2889\n",
      "Epoch 18/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 4.6802 - mae: 4.7212 - mse: 89.4027\n",
      "Epoch 19/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 4.6256 - mae: 4.6689 - mse: 87.0266\n",
      "Epoch 20/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 4.5345 - mae: 4.5718 - mse: 84.3092\n",
      "Epoch 21/40\n",
      "191/200 [===========================>..] - ETA: 0s - loss: 4.6006 - mae: 4.6433 - mse: 85.2581\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 4.5898 - mae: 4.6302 - mse: 83.8792\n",
      "Epoch 22/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 4.4807 - mae: 4.5274 - mse: 81.8220\n",
      "Epoch 23/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 4.3508 - mae: 4.3880 - mse: 79.1739\n",
      "Epoch 24/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 4.4086 - mae: 4.4508 - mse: 83.2040\n",
      "Epoch 25/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 4.2758 - mae: 4.3077 - mse: 78.6112\n",
      "Epoch 26/40\n",
      "193/200 [===========================>..] - ETA: 0s - loss: 4.1794 - mae: 4.2179 - mse: 71.9503\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.00032768002711236477.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 4.2295 - mae: 4.2672 - mse: 74.7733\n",
      "Epoch 27/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 4.1640 - mae: 4.2013 - mse: 75.5137\n",
      "Epoch 28/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 4.2056 - mae: 4.2342 - mse: 73.3027\n",
      "Epoch 29/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 4.1575 - mae: 4.1976 - mse: 74.9901\n",
      "Epoch 30/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 4.1223 - mae: 4.1568 - mse: 72.6326\n",
      "Epoch 31/40\n",
      "199/200 [============================>.] - ETA: 0s - loss: 4.0052 - mae: 4.0377 - mse: 66.7850\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0002621440216898918.\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 4.0081 - mae: 4.0403 - mse: 66.6782\n",
      "Epoch 32/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 3.9303 - mae: 3.9651 - mse: 69.8084\n",
      "Epoch 33/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 3.9445 - mae: 3.9787 - mse: 68.6844\n",
      "Epoch 34/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 3.9617 - mae: 3.9977 - mse: 66.8294\n",
      "Epoch 35/40\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 3.8992 - mae: 3.9350 - mse: 67.5253\n",
      "Epoch 36/40\n",
      "196/200 [============================>.] - ETA: 0s - loss: 3.9173 - mae: 3.9508 - mse: 68.4917\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.00020971521735191345.\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 3.8830 - mae: 3.9152 - mse: 67.3600\n",
      "Epoch 37/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 3.9735 - mae: 4.0048 - mse: 68.6345\n",
      "Epoch 38/40\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 3.8657 - mae: 3.8924 - mse: 65.4698\n",
      "Epoch 39/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 3.7490 - mae: 3.7783 - mse: 61.9259\n",
      "Epoch 40/40\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 3.8592 - mae: 3.8839 - mse: 66.0393\n"
     ]
    }
   ],
   "source": [
    "train_iq_data_single = tf.data.Dataset.from_tensor_slices((trimed_iq_x, iq_train_y))\n",
    "train_iq_data_single = train_iq_data_single.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "EPOCHS = 40\n",
    "# opt = tf.keras.optimizers.Adam(learning_rate=0.1, beta_1=0.9, beta_2=0.9999, amsgrad=False)\n",
    "opt = tf.keras.optimizers.RMSprop(\n",
    "    learning_rate=0.001, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False,\n",
    "    name='RMSprop'\n",
    ")\n",
    "iq_model = build_model(optimizer=opt, nodes=70, input_shape=trimed_iq_x.shape[-1])\n",
    "train_iq_data_single = tf.data.Dataset.from_tensor_slices((trimed_iq_x, iq_train_y))\n",
    "train_iq_data_single = train_iq_data_single.cache().batch(BATCH_SIZE).repeat()\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"mae\", factor=0.8, patience=5, min_lr=1e-6, verbose=1,\n",
    "                                                     mode=\"max\")\n",
    "history = iq_model.fit(\n",
    "    train_iq_data_single,\n",
    "    epochs=EPOCHS, \n",
    "    steps_per_epoch=EVALUATION_INTERVAL,\n",
    "    verbose=1,\n",
    "    callbacks=[tensorboard_callback, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(260, 461)\n",
      "260\n"
     ]
    }
   ],
   "source": [
    "file = train_file\n",
    "file = test_file\n",
    "\n",
    "org_sj_test_data, test_scale, sj_columns = generate_lstm_data(\n",
    "    file, history_size=52, \n",
    "    cols=new_sj_cols_no_label, \n",
    "    norm_cols=new_sj_norm, \n",
    "    scale_cols=new_sj_scale, \n",
    "    single_step=True, \n",
    "    train_frac=1.0, train_scale=sj_norm_scale, \n",
    "    prepend_with_file=train_file,\n",
    "    extra_columns=extra_sj_cols,\n",
    "    group_by_column=True\n",
    ")\n",
    "org_iq_test_data, test_scale, iq_columns = generate_lstm_data(\n",
    "    file, history_size=52, \n",
    "    cols=new_iq_cols_no_label, \n",
    "    norm_cols=new_iq_norm, \n",
    "    scale_cols=new_iq_scale, \n",
    "    single_step=True, \n",
    "    train_frac=1.0, train_scale=iq_norm_scale, \n",
    "    prepend_with_file=train_file,\n",
    "    extra_columns=extra_iq_cols,\n",
    "    group_by_column=True\n",
    ")\n",
    "sj_test_x, sj_test_y = org_sj_test_data[0]\n",
    "sj_test_x = np.array(sj_test_x)\n",
    "sj_test_y = np.array(sj_test_y)\n",
    "iq_test_x, iq_test_y = org_iq_test_data[1]\n",
    "iq_test_x = np.array(iq_test_x)\n",
    "iq_test_y = np.array(iq_test_y)\n",
    "trimed_test_sj_x = trim_data(sj_test_x, sj_columns, sj_col_size)\n",
    "trimed_test_iq_x = trim_data(iq_test_x, iq_columns, iq_col_size)\n",
    "sj_test_set = tf.data.Dataset.from_tensor_slices((trimed_test_sj_x, sj_test_y)).batch(len(sj_test_y))\n",
    "print(trimed_test_sj_x.shape)\n",
    "\n",
    "sj_pred = []\n",
    "for x, y in sj_test_set.take(1):\n",
    "    predictions = sj_model.predict(x)\n",
    "    sj_pred = predictions.flatten()\n",
    "    print(len(predictions.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156\n"
     ]
    }
   ],
   "source": [
    "iq_test_set = tf.data.Dataset.from_tensor_slices((trimed_test_iq_x, iq_test_y)).batch(len(iq_test_y))\n",
    "iq_pred = []\n",
    "for x, y in iq_test_set.take(1):\n",
    "    predictions = iq_model.predict(x)\n",
    "    iq_pred = predictions.flatten()\n",
    "    print(len(predictions.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416\n",
      "asas\n"
     ]
    }
   ],
   "source": [
    "from preprocessing_helpers import export_test_to_csv\n",
    "preds = np.concatenate((sj_pred, iq_pred), axis=None)\n",
    "export_test_to_csv(predictions=preds,path=file, prefix='test' if file == test_file else 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwQAAACnCAYAAACvm5s/AAAABHNCSVQICAgIfAhkiAAAABl0RVh0U29mdHdhcmUAZ25vbWUtc2NyZWVuc2hvdO8Dvz4AACAASURBVHic7d1/dFzVfff792hGOiMLxj9ghFQ8BcvCqaRAo7nkCUPpgyJuLWKKCD/shxvb7SpyWMUi64Lj2+LCTdQsEyfrMXbXxXJug8XTIJMLMj9i8eAg3yKUllq+LRm1kJGLGcc4I7DROLZ1bNlzpJHO/WNGtiTrx0iWLJH5vNbyAs2cc2bPmb332d9z9g+Hbds2IiIiIiKSljJmOgEiIiIiIjJzFBCIiIiIiKQxBQQiIiIiImlMAYGIiIiISBpTQCAiIiIiksYUEIiIiIiIpDEFBCIiIiIiaUwBgYiIiIhIGlNAICIiIiKSxhQQiIiIiIikMQUEIiIiIiJpTAGBiIiIiEgaU0AgIiIiIpLGFBCIiIiIiKQx18R3sQjvrWfn6y0EPwwTOW5iWhbEwcgxMAwPxpy55C5cyMKFPgpvWUX1vYUYU592iAfZtHQ5Oz4pYd2eRqoXT8eHTI/Q1goqt4XxPFDH/h+WTc/5mbQI9SsrqNlvUPbDvdQ94J3pBImIiIjINJlgQGAR/MFyVj0Xwhrp3W4LqzsKJ6JEO8KEgNa+MtbdWzgVaf2dYh7vAsA62YkFsysgiJtET1iAhXkiNtOpEREREZFpNLGA4FA9Nf8jhIWBb2k166oqCBT48HgMjEk8a0hrzoH/umdXMADgGvR7zrrEiYiIiMhUmlAzPvpuC6E4sHgNtc9WU6IgYNICj+1k19dMjOtLZmGbu5DVz+4icNzAW+Sb6cSIiIiIyDSaUJM+8nEHAN4vBxQMXKoFhfgDM52I0Xmu9+O/fqZTISIiIiLTbUKzDFk9if7kc+d7piUxIiIiIiJyeU3oPr8xxw2AFR9pSPEExFupKVtFfdTPhrd3sWbhGNserWdVWQ2t81ez890aAiOm2MAY6JNvRWh9ZSf1jS0ED0SIWuC5thD/7cupXrsa/4gT5oSpvbuCLd1r2LV3A34XgEn47d3s+lkjLe+HiRyzwOsjUL6K6sdW419wYW+ro5XdLzXQ+HYboU86MfFQeONXWf7oOtYERp6hJ/L8csqeDuJ7aBctT/qHvWsRebueuleaaG2P0HnSxOwedM4NA0+Oh7nzc/EuLGH1dzZRef2wIxxsYkddAy3vHyJyrJPo2cRMUEBijMAcA48nl9y8hQSqfsCGpYPTadH4SCmP7zWo/HEbW+8Y8StA3CT0Rj279rbQ2h4mEjWxLDA8XnIXFlJ8cxn3/G8rqFgySgB5ooFVgQ0El26n7dmKZNcpi2hbE7sb36JpfzvhTyKYcQ++JQEqqtax7u5RZqw6EaLh+R00vpvcx7Sw4hfeNnI8eDxzmXv1Qhb/r9VsfTQwC7tqiYiIiFx+EwoIfNcvBCJ0Hgxj4mdWPSdwWUTfruWJ7+6g5agFOV58eT58ZoTIkRAtL4Ro+XkLm16qY8X1oxzjrIkFmO0NbPnOJurbTMjxUnidj0Kjk/ChMC0v1tD6vsmuV6opscI0PlvD919oJWoZeBcX4isw6Px1mPD+Bja9F6TjH3ZRE5jYmYq+/jjL1zcRBTA8eL25FF47UvPVwozGiA3/FQ/tYNV/20TQBFwGHm8uhV7jwkDmwUc4G8WaTNP4RCtbHl5LbZuZ+DvHS+HiEjwGmCc6iRxsJdLeStNPdxD49na2Pzx6frG6u7AAq72B2h9up/7dCJbLwOP14SsowfwkTOSDJnY81kq4u5G6B4eNa7CCbFm5itqDiaDJWOAl97q5F4LEIduaRGOGggERERGRpAkFBN7by/AbrQT/pY4t795JzW2zJCRwGnTsrGLpc60YN6+m5gdVrLjNd77RZ37QwKa/qqHhYAubvt/AV3+8ghHv25/tJPjy42x4upHOhRVU/101a75Wgsc1cJwdrH1oE60f7KD2eS9z36ih4VAuZX+2leqqygtPH7pD7Fi7ik3vhml47i2qA6N83ojCNDzfRBQD/yN1bH8sgHeC4zWCO3cSNMFz2wZ2bl1DyYLx95mYKI1PJoOBhWWs+9sa1pT5hjayT4Rper6G7z7XSusP17Jp8V423TFKfjE7eGtrFVv+7xai8/2s+OsNrLq3gpKBkxaP0vL0Kta+EKbl+Z2EHtgwZAyL9Yt66g9akF/Bpue3smKJmvsiIiIiqZrYSsULV7NhZSFGPEx91e1UfmsTO15qovVgdEj3jMvOaqX+p1HKvtfI3pdrWH3b0Map58YVbNpWjd8A8xe72P3xKMfpbmHL91pZ+FAdTW9sZ93dF4KBxHHWsO7rPsCk6YcbeMtYzfY3mqh7onJoV6ScEtY8tgIfYH3QlpiZKVXxQ4SPAEaA5Q9PPBgAk/CvI4CHsj9bPQ3BANBeT22zCUYJ656ro3p4MACwoJCK9XVs/4tCIMruH+8mMtrx2mrZ8NwhSr65naa3d7Hp4UHBAIDLS9m3qggYwJEQoZNDd48cOoQJ+P50jYIBERERkQmaYHPTwP/kTuq832fTc42E9uwgtGfgLQ++JSUU3+THf5OfwO0BSryXq3HmZcX/tYtNZWM8sVi8gsobtxB8L0TreyZrrh9528B33mDnytHv5xcuTnSb4pYaGn+yGt9oZ3DxYha7IJLsDpOyPhLBlcvD3DkT2XHAQN95g7lzp+f8h3/RQjgOxh+tYvWSsbY08K9ajv9/bCL4QQutJ1bjGylAcflZ97OdVI/VmPf4EvtGLcxuGPLIJZY4w54rZskTKxEREZHPkYk9IQDAS+DhrTS2ttHy8nY2PVnN6rvLKMmDzg9aaXqxlk1/XUXlbbdQ+WQD4Uscf5wSl4/FheM1Br0UFnsBi0PhUe5Vu3yU3DRO5x5XotHqXVw4ejAAYLhH7sM+HqcXXx5gtRM6MIn98eDL9wBRQu+Pek/+Elgcag8DUOhPYRxJnh//tYAVov3DUbaZU0hJwXjBizHq+cz1LcQAIqF2zPHSIyIiIiJDTCIgSHJ58N1cwYqH1lHzd3U0NrfR/v5+Gndup+aRCgrnmIRe2sCq9Y2JwbHTLYVuOd7cXAC6jl+WFE2Oy0/l3YUQD1P7yCpqnm+ktT2CmXK3I4PA1+/EBwQ3r6bqB/U0vRcm2j1VCeyks9MCDLx5ueNv7vKx0Atg0nl8eqJDz+33ULYAzL1PsPxbW2h4O0g4ejkiUREREZHPv6ldXizHS0mggpJABSsqa1n19S0E99ZS317JuuIp/aRJmTsnObGlZZFo0s5OJd/awdaT6/j+y63UP91KPYDLwFtQQskXSigpLqbE7yfwpcIhYxwGGLfVUPt0jA0/bKTluRpangMw8CwspKR4MYXJrl2BL/vxTbiXjYV5Nvk5c1LZ2cDISexndU/TWfdW8oMfdcJfbaFpTy0b9tQmPnmBj8LiYkqWlFBcXIL/lgAl+bP1VxcRERGZGdO23rCxZDXLb6sl+HaY1v0RKPaNv9M0i/Ul/+dS11GYbi4fld/bReVjYVr3Bwm1txM6GKL9wzAtbwRpeSO5ndfP6ie3sOHu4YN6DUoe3ErjvX9DqLWV4IHEvuH2EMHmEK17G5Ob+Sj7Zg0/+FbZJAYvzy6em9ewvXk1kbZW2t4PEWoPEfqwnXBbE6F3m5JbGRQuW8emv10zZB0JERERkXQ2jc1AD4XX5gIRIh93ADMfEFhnk/PU53hm7dOBIRYUElhWSGDZoNe6o4Tfb6Xlf9az45Ug9evX4FnYxLrSEfY3vJSUVVJSVnnhtbhJ5GCQ1ubd7KxrpGXbWtbObWTXQ4UpJsrAMwfAwjprwrijCLqwuhP7GTnTfdYNfKVl+ErLuPCNLcyPwwT3N9Hw/A6a9myiKp7L3h9VTmAqWBEREZHfXZMfQ5ACw7jQRWc26IgkBtnm5n2Om4I5XgoDlax5ehd1f1kC8TCNjcHU93d58BWXseLRrex6djU+LIKNTYRTPkAuubmJ3zV6rHP8zeNROo4BeMi9eibCMAPP9SWUPbiO7S/9gIoFYP6igXdOzEBSRERERGahaQ0IoicTDca5F/U1H9QwHGewrHXoEB3jDqhNIeCIhwi+bwIGi78w808rpkJJaQkeoPN4dGJTmyYZN/lZbADHokQnMGh5cXHiaUI4GBx/Vp9jQYLHAKOE4lQfQkyXBaX4rwP6BoIUEREREZm+gMAK0vJviQb4wsLhDfBkt5N4hEMdozRlrQgt26qoeLh+9AWtJsB8dyeNBwEjQNktvxvz1ZsnOxOBgGFMrguU2YnZN/H9C28vo9AF1v6d1B8ca0uL4M5dBONg3BggMNMPZuIm0RMABsbnos+YiIiIyPSb+oAgbhFtb6L2W+uoPwLk+KkoH9YSdBVSUpyYK3/339XSOngWUDNMy4ubqFpaQdXWIN4HqqlcPN5ndhL8l9DIU2vGTcJ7trD2/2ggAvjuXcM9M90wnQJWRxNbnm/FAkqKxztBIzDDNGzemWisf6GEwomMJileTXW5B6wQW765ltqWyMVPCk6Eadpaxdr/EQa83PPwipkdRRI3Cb2whd1HAE8JJQtnMjEiIiIis8fEBhV/vINVq2oJjdY/JW5hmoPedHkp++saVucP39CgrGo1JXtrCbXVsuq2HXiv9eHu6aQzamLFwbOkknV1f0N1GdT/t1oaR+mbYvUBRGn6TiVN3zHweHPJnZ+8/dtn0XkskljZFvDcso4tfxOY9QOKwz9ZS82eUboBWRbm8QiRo2bi/etWUP31YU3ttlqqNreMvHZBn4V5opPOT6KJ940S1jxSMf4CY0N4qXx6O+HoWmrbmthS1cSWHC++a3PxGGCZESJHkulzeQms386GO6bxqYzZwqZv1RKMjfBeH1jdUTo/iSQDRg+BR6oom+2ZQEREROQymeAsQ7kU3liC+WGYyPHo+Yb2haMZGAu85OYVUlwa4M4HV1NZPHJD0LhxHTtf8LLp2Z280xYmeiyC1+vDv3QFZXdXsmJpSbKRajJ37iitN5efmj37Wb7/HVrb2gkdDHPo4w4in0Qwu5MzCi3wUVJaStndq1n9df/nYnpNy7KwTkYJH+s8/z3OMww8nlwKb/ZTcss9rPrzSkqGTaFpdVtY3SaRTyJETwzb32VgeDzkLgkQKA1wz5+voWLxJFrHCwKse2kvFW80sHNvC8EPwkSOhIhYiVmccpcEKP5yGfc8uIKKUfLAlLEsrLhF9JPI+YByMCPHgyevhEBxgIqvr2JF2e/GGBIRERGRqeCwbdue6USIiIiIiMjMmNZZhkREREREZHZTQCAiIiIiksYUEIiIiIiIpDEFBCIiIiIiaUwBgYiIiIhIGlNAICIiIiKSxhQQiIiIiIikMQUEIiIiIiJpTAGBiIiIiEgaU0AgIiIiIpLGFBCIiIiIiKQxBQQiIiIiImlMAYGIiIiISBpTQCAiIiIiksYUEIiIiIiIpDEFBCIiIiIiaUwBgYiIiIhIGlNAICIiIiKSxhQQiIiIiIikMQUEIiIiIiJpTAGBiIiIiEgaU0AgIiIiIpLGFBCIiIiIiKQxBQQiIiIiImlMAYGIiIiISBpTQCAiIiIiksYUEIiIiIiIpDEFBCIiIiIiaUwBgYiIiIhIGlNAICIiIiKSxhQQiIiIiIikMQUEIiIiIiJpTAGBiIiIiEgac6W85ZFusv81TmzQS/OyHPjzDTb7DUqHHMmm7cOzrGyPcyA+9DDlX7yCt4uc57c78OtzPHqgl+A5OAW4nZCfnUFV6RU8eY0jsdmpc/j/sYc2e5S0ZWXy9t1zKFd4I5dLdy/PvB+jrrOfwz0QA+ZlZVB9yxVsHMi39PF00xk2znFz8o8N3Od3ttnTanLXqSyCFdmUns+3/dS9c5o1xwd9jgMWXeHkviVuNha4Bh1jItsCZ3tYvy9G7Ul7SBkm08WblTksG0hDfy+PvnmW2tjwsgqxI93M/7d+Nt95JdVXJF479tEZFv1739BjDuLOG/7dh+nrYeXr5/jpoLLtdsEij4vqP5xD9dWOEXayaX7vNHcdtolluXi1Iof7RviAYwfPsOg/+iA7k1eXzmFZ1sB3jPP03m62LZjD0f+SOTQtPzvHgSU5BG8cqND6eW3faVb+1sWrS3NYZoz2RdJcXw9r/meMvNuuZONVDvjtOfzv9rPxT3NY5hx/94FrwVMfxWk+bXPKBjKg6Bo3b95msGjQlqc6YzwV6mHPKZujfeA2HNyab7DxJoPSrGGH7TxL8T/1csCGQFEO+744+ELVR+07Z3j0OIly8Kc5LDv/tk1r8DS3HhqUMR2Q586gvMBNbXEm8wZ/Tirla0LXsX7qWk7zaJ/B4Tvc5CXT9No+k/s/Gf0slt90BW9/YdAJ7+7h6fctXuzs53Av4HJQelUm6//QzX2eYWVrhGu8O9NBaW4mG/3ZlA8uY7Fenn4vRt3xfo72Js6N23BQdLXBi4Ghv1faSPWcxCzu+p8xYjcO+636e1m/5ywv5o5QL11URzoomu/iKX/2sN8xxTbVBB1oP40/1E/M4eDbf3wlmwcfx+ph5c/P8dNemHeNmwP/1Ujm18FSq7P5pJv5++KcGvTSQB586g/dLMsZlv5Ur2uSktQDgiwH+cCiG7L5thfoszl8vJdnDsWonuMcWtH29LK5Pc7J+Vm8eoOTeed/Qwfz5w76hawenvqPXoLZLqqLneQPlA2Hg6LBP/ycTDaWZnC0H7D7qHu/l7YrM9lWkNzBlUGxfni5XPriPP3uWZ4642DZ72dR5XGcb/AWXTG5Cvc8B3CFix1fzCQ/A2I9/TQfsngmeI5FuRca4hPb1uZAOMYzp6DqS9l8I2fQMZwZ+AeXHdvmZA/kuWFf2KL5hjmUj1FLzMvNYtuXkleq7l6e+qiP/IXGhYb8Fa7Rg4FB8q4xqFucAf0QOxen7kAvj/5bjFuHBExJfXFe+8SmdFEm8yO9vPhpP/cVXFwBnOyxwQH58V6eOdzHsi+k1DId4tSnMdZ/Csv82QoGxnKun8P9GZRfmfjdY2Yfh3NcFKV4yo8dPkv5L+PgcVH9B07yB9pDWU7yB2/YFeP+f7bYl+WkqtBFqQHHftvLtsMxlsWg7bahjZHYOZvDOFjmheZPejnwRRdF59+Ms+cELPNm0Hzc5mgPg66IDuZlOXA7HFR/2UiUgbjNviMWT4fOkX+Va1CjKMXydcnXMQeli9zs8Cb+OnXcYn2HzbIb3NyX/Mx876AD9PXy1D+f4+luB/ctMqi+EmJn+njx4x7u/yebt5fOoXxwADX8Gt8PR0/18MyHPax0uTj8XzKTZdmm7T/P8dRRm/JFWXz7ygt1ntvtZP5YX+F31vSfk7w8g7qCRB156kycbR/2sjLo5HDZoDyfaptqQmyOdtvgdrIss4/XPomz+ZoLAUuss5dmO4NlC/ppPtvPSbg4IEixzsblYD7gL8zm21cDNhw1e6n9qIf7/xma/ySbwPk6ZQLXNUlJ6gFBBuBwUJqfybKBivD3XcSiZ3jqVD8xuHDhj/dzqg+K87O479oxrgjd/Rzug2VFc9h43RiZNcvFssXJpPbHOfxRLwfmuqi6YfjtIJHpF/vMYpsJ5V/M4c1UWzwT4HY7ucuXdb5SvW9OP3v+qZd9J22qhwUcqW57tAfIdPGNxVkpPUlbdG0W+Ud6eOZIP+WLR9/BPTeLqrnJP0ybunAf+ddkUTVSZT8aB+TPd7Hs2oHqKIsi6zTNH/YR7IHS4RHF8V729GZQVWiQd7qX9Z/GOVWQNfSO7QCni6rf72Nj2KK1cM6gi0kK4r1s/I9eTl5lsHki3ycddfXR7nZS5AKwOdDVj/uKjBTvFPfx6qE4x9yZvP3VYY3UIWxaf91Ds53BxltzePKqgfxtUPqvp7nrNz28esYYEjTH+hKPGpZdl8GBYB/7zkLRnOR7nXH2Zbiovdam+bjNqf6RPtPBrddmnX9ysMxr07bHYl+0H665kJlSKl9TcB1blG9Qlfz/U85envrEprzAoMpz8baxYz3UnYbym3J4dVAwvHLuGYp+2UvtJ/2ULxqU2AzAAUXXZLIsP3lufS44fppHT8Y5TOb5YOrwaRuuzGKbP5siFQ1gms+JAxYNqSMzWXQuTnmkj/Z+yBv4vFTbVBMU67Mhy8nKa/qp+izOgfN5wabtsz5OzctiZY5FcxRi/VzcGX0idfZAHvy9gfRncZdxhqJ/76HumJvAtZO/rsnYLvkUuscKKcbLj85EEHEqPtozVJHZ5/Bv+znlyOA+39QHAyNygtsB2VO97ViyMqn2OWj+yKJtxIbS9HKPemptWj+JczTHxTKPk/JrMogd72VffJTN+6FocRblPb1s65jIF7Fpa49RezaDp0rTtAtEKrpi3PpqF9n74hw708Otr3WR/YqJ/yObY5FzZL9+mtoz4xyjr499pyHvGtcYwQCATfspG3JcLJs/+OLioPxaJ3l2P20nh15LYsl8MT/XRSCjj+aoff5Y+z6LE5vvIuBOPOGLjZaHBjMyyHfCqd7Zf806fCJZTw27KZd3bSblGXDgZF9Kx8l2AhnDbkQ4gT57SNeOdHe5z4nbxcUtuGlqU8X6AaeD0mtc5J/po7k7+UZ/H83HbUq9Lha5HNBvj9B9dIJ19gjy8l34HRA8kVqelcm5hIDA5tTxXl4z4dZc58jdAmyg377wb7grMym/Avb8qptHf2Wx57ej90UWmR1sjsb6iTkyyL/kVvco+uGk1c+pnn5Odcf56a/jHHY6CSwYIcKeyLYA9hjlcZjyJVmUdvdQe3T6Gz+xuJ34Dj39HOvqofaTfrjSya3DK5b+Pt78zCbf66I0Axb9nouieJzXPhsjjVcYVOfDax/1cDiFtMx3OYidslj/UT9FN2RTPW/q7rT9zpnrZt/9Ht68DopuyOHcA3M5d18OG6+Eqls9nLt3WDe3kcT6OdaX6Oc8pv5+jlngdjvIH7apOzuDfAccjQ3NB+f6bHA4cLtdlM+D5s+SfeT7+2g+DqW5LvKdDtzYiW3H02dzsn+MgHUC5Wt6jVFPZWaQn3XxuRoQ602WRaufw59ZvHgC8uc7L3S1wsGt+S7yzvZS9S/nqD3Sy2Frpr/vTLu85yR2Ns6rR23mzXcN7RozTW2qWB/ghPlXJQPrzuTNlbNxmrsdiTZgBtBnc274zpOps4fLziDPAadGybOzp9x9vqXeZajP5pxt88w/mTwz6OWia928Wjhy7dj8/hkc71/4231lFsE7sy9ULBkuNt6WTfb7FnUfxag9kBhAsuw6N5tvymLRZboBKzIRsT4gg5T6xk/q+Ccsihut83/Pu9LFxluyqcq5tG3p6eWO13qHvPSNr3h48fdHafB6slifb1F10GLjte6RH+9OBRsOhM8yP3zhpaJrsnjV7x7UCEk61cuebggUOznV0w/ZLm5197Dn0zixazNH+U0cLFuSRVFzD7WfGWz2jp2c5l+dIftXgNvFm8WpjYFIb/0cOA1FA91P+vtpP5eB/8oUA6nkNdztHH/7c32A03Hxb5J8LdY/whOCDHCTwa3eDE4diXOgP5PSM4mGTHluBu7+xBO10Z4QxOLJu559/ew72ENzPyybN8LFaaLla5qNVU+5MxJ3fYd09e2xOWlD3f93mrqB1xxQvtjNmzdmDtl/3qJs9sRjPHWoh/X/2sOjJLq0fPtL2VRfnZ59N6b1nNjQeqAbx4ELL+V5MqktHdbtZlraVDbn4iTKWJaL8vnwVDRObFEWsc44QaeTb1/lwN3pgD4SEwIMNqk6+2LZGYwctM+ycvd5lnpA4HSQ7YD7bsphY27ipVNdvWz89xj3BzMI3nzxD1u6OJu66wYVBNcIfUqvyOLJW7N4sr+fA9Fe9vymh83hc6xxO4fMcCIyW7idJAa/prLtZD7gykxe/MNM8jLg1Ike1ocSfekvedsMJ5v/2E35oGI1b8xGWwb3LcniqZYetv3W4KnJfJcU5eUbvHiDE/ptDh+J8eixPo72X5y2A5/GOWBD27+e5qeD3/isl7b+TAKjXXcXGKzP7aH6YA9PeceuV0oXZVM7v5eqtjgb/zNO+RcVFIypv58DZxwUDcx2YvZxwJHBypGC0pEkd4ulcIc+O9kt46Kyl3xt/pCuLXbimBkO3A4oynOy6D/jNJ+xWRSN05bpZON8B5xKPCGIjdQbwe5j5RvmkJcWeQ3WXztCuZlw+ZpeY9VTsf5EUDAkXw8M6PyDOTyZC/T1s6c9MWtO7KKvkUHpDXN48waIdcdp/rSXuoM9PPov51h01+DZmtLJBM/JBLtiFl2XzatLEhVc7FwfL4ZiVO07x6I7hk28MOVtqmTZyHTgJoPy3AxOHY7T1p/Jyc/64Co3tzrhpBOwbYbfxJ90nT3MuX7IHummwSwrd59nEyy2DhbNdVI08Ah9npOnTvRSfqSHfaWZQ34QgPlXOCm9KsUMmJFB0TUGRdc4if22m43ROKeKnNN3V1JkUhzkuzNw2/0cPgeM0x3CnZyd5BSDZ16wOdWbvCCPUBm6jQzK8zMT21+TwdFPz7A+3MOx69wXzd4wkW1xZVB6teviWXvGcnUW1Vf1sPFgL49ePJfc1HBA/lwX5dckq6MFNnv2xKj9OE7VTUOnidxztB88mbx6Uybzkt8j9lmM+z+M8+oJm8CI05QCZHDfDZk8ta+HWnOcvl5GBoHFc3ix+wy3fniOZ37vCp4crQtWWuuj9u0zrD9JohHQknx6bCcaofe/3kVp8RXsKx7nGuDOIM8J7d3jtJAyMsgzIBazOTp4ICUQO9fPURv87qG/UywOOJNjaua7uDWzl+bP+lkU7cO9wI3fCbhgHjBil3qHk41/bFDuAHAwb04GRVeMUoAmU76mzYV66ujweqq3n6M9kD/sXJ0fVHy1i/LkxCG30sdr+3qp63SzLX/kMuDOcbHsBhflWf3k/1sf+07ZLBu1HKaHMc+JKxGIHR3ez9+2ORVP3Am/iINE3puXrA/nuSjqj/Niay+vnnRTetVIDeWpalPZnOpLBJhuYNE1Lha1x9lnxjn6W/Df4Eoc05VIw7k4g1qWl1JnFvnJswAAHZRJREFUD3Kun2M2zBueZ2GWlbvPt0sfVEzikeqxkW7uTLY7l4MJR88il8uiqzKYZ/ezJzLeAKcMFuUkpmDcN/iufTzOvi5w5zhTGKyawa1eB5yK02xN5bapclK1xEX2pxYvXtQ5dJoYLso9cKAzPrTP/5le9nRBeYGb+/IzKb8m8W/ZHxjc57Jp7hj793DnG1TP7afuYPzifq4XcVBaks23c/rZ+MsYbRrLNgIn1XfM5dwfZbJoTib77pvLuQfmEvyDDPIWZnPygbnjBwMATif+K+FYtJc9oz3dAsBB8TwHdMfZM6Rfgk3zJ30cc2RQOn9og+Fcv32hi5HTRfkCCEZ7aD4J/muSDRmnY8wuQ0VXZRLIzSSQ6xo9GJiFFi1I1FOvfTI08x77pJfmfiiaP/5v477aRcBp0/xZCgVg4EmPrt0XjHROMpwUZcOB3/ZxbPC2p/oIxmHRlRPIY7bNqVTq+ktpU/Un7s6f79I3z0V5Zj/Nh3vZZzkoT051mxhXYw8tR5dYZw849mmcoA3+Beo1Mp1Sf0LQD9g2bUd72ZP8wWPdcep+Y0O2c+g0W64M5jmh+WgPr13pYvCYvPkeF6Xn58Pto/nXcQ6fzxM2p7ri1J2GRbl6OiCzk/sag6or4zwd6uau05mUX5nsv9xn477KoPr8iEcH5ddlsijSS/U/dXPA5yKPfto6eqiNwTd+P5WuKA5KvS7yD/bS3GnzDd9Yd1NG3zY/C9y9cV473ANzBh3DmYF/nLLmzjeoyulm86GR5pObDhmUex3wUZzmGFQlT9KxT+PsI4PNw0eUZrm46yp47VgvB3BdPO7gPCdVN7jY/J7FT1O5rjhdPHVzFntaeqhuz6T5RnUdGskxs4+TV2YlrwE2B073s8gzykQTI3KycrGLze/FWdncTdVC5/m+zqfiDu4rMZLHdhAoyKL81xYb/6Wbo9cPWoegwyYvz+D+YU/szvejzwDIoPwaJ6f+o4daRwYbB6YNzXCcv7E1/tR4I7uU8jVd3HlZVF0R5+lfdXN/dxbl59ch6ONUdibV16ZQlrOc3OqB16JxDuM6fwPj1IkeXv3toKAs3k/zkTinnE78wxc8SxMpn5MMJ/f7MnjmwxjL9vVTdXUGxPvY8+te2pxOdow0VbsNh0/G2fNJolUf6+mnORznmCODRUPOd+ptqgO/Os2th528etd4i7omugydf3LhdFHuhfvDvZCdyTMDDTyX4/y2AyZVZ9sQPNbDa8lA5+jpOHWH+ohdkUVV3tC8NRvL3edZ6gFBj81R4PBH52j+KPlaBhR5XGy+yRj6uCYrk/XFPbS193D/vwy95TNk9dN4H3sOxqg7c2EgijvTQenvGTxTnJadEOXzwOli4x9nk/0fFi9+2sP65KqUedkZ3DVn6LyJ7vxs3rwZnvowzuYPEisw5mVnUPWH2Wz+/RQb11e5CDh7aT4WB1/mJLZ1UFToprrzHLXBc9QO3j6VFR0zXFR9wcW29+LEMhxjTzU8JRwUeV3kf9hL82c2Vdclbm+9+WkfXJlF+UXdtDIoz3PC+3H2dEHR3BEOmTRvocGjB+I8dRqKxjmVAO6r3dQW9lJ+MNl1aKRH82nNpt20yb8iI3nx7ePAaVjkm1jgmLdoDnv6EysV1/1nnFN2cjXWeZkE+rlww2mum1f/mMRKxWGL2niiG8Gti9xsvOniFVJjA4MhBz6ncA7tuf3EMjIoGpi7P9mF49SkZyi5xPI1XZyZbPyviQGmL0asxNMXl4PSq7N49Q/d40zxOiAZnB+M03wWqpJrOByN9vDMgT4ODIzlTK6S/m1/Nt9I06g59XPioPSLc3iVGBt/08P6TyHmgKK5Ljb/L9lUjdIN9dgxi7uOnT/E+evIkFm8JtimSvVpTqxv8KB/B/fdfAXtf5BYrGygbLqdMA+bk+cD60nU2XGbk8DhQzHuZ1D6vVm8+iX3sHVkZmm5+xxz2LateZpERERERNKU4icRERERkTSmgEBEREREJI0pIBARERERSWMKCERERERE0pgCAhERERGRNKaAQEREREQkjSkgEBERERFJYwoIRERERETSmAICEREREZE0poBARERERCSNKSAQEREREUljCghERERERNKYAgIRERERkTSmgEBEREREJI0pIBARERERSWMKCERERERE0pgr1Q0du7qmMx0iE2Yvnzsjn6uyILONyoJIgsqCSMJEy4LDtm17mtIiIiIiIiKznLoMiYiIiIikMQUEIiIiIiJpTAGBiIiIiEgaU0AgIiIiIpLGFBCIiIiIiKQxBQQiIiIiImlMAYGIiIiISBpTQCAiIiIiksYUEIiIiIiIpDEFBCIiIiIiaUwBgYiIiIhIGlNAICIiIiKSxhQQiIiIiIikMQUEIiIiIiJpTAGBiIiIiEgaU0AgIiIiIpLGFBCIiIiIiKQxBQQiIiIiImlMAYGIiIiISBpTQCAiIiIiksYUEIiIiIiIpDEFBCIiIiIiaUwBgYiIiIhIGlNAICIiIiKSxhQQiIiIiIikMQUEIiIiIiJpTAGBiIiIiEgac810AkRERERkhnS0UPt3tTS+GyIctYa85bljE3t/vALvDCVNLp/JBQTxKKGfN7DzpV3s3h/BwsuKn+xn021DN2tZX0rV6+aEDu25sZq6l9fhN8bZ8FAtlcu2EIqncFBXIdWvNLHuxqlNW/S9Rna3BAm2hzh0MEwkamINpMcFxhwvuXk+Fpf4KfvTVawo8zHm1zoRpulnO2l4aTcth0wwKtjatp3K8c6FzJwUy8Jg5sct7H6pgV2vNhE6Adxcw/6XV49R4ZqE3qhn154WWtuT+cwCI8eDJ6+QktIAFctXs+JmVdkyk6Y2n5oHm2jYuZum/UHCx6KYloHhycW3pJiypStY9WAZvgnWjebbG1j6cANm6QYaX1lD4US/YneYlpd30fB2K+0Hw3SaFhbJuv66QkpvqeCeB1dQdr0q7XRkfdzK7r0ttH0QIvRhmEhHFHNQ+9rI8eC52oevsIRA+XJWP+DHO9O3ZU+0sGFlFQ0dgMeH//ZivB7jfFvFfVPhOMFAlNaf7KCxLUyko4OOaBcx08S0rAvlf34uvi/4CZRXsvreAF4Vj1lpQlnRPJRsyPws2ZAZh3exn5LiaGoHj3cSPhjF/HWYDovxA4KziYqYBT5K8jxjb+tczML5U502i+DOJ9j0RqK0Gx4vnjwf7oF9+mJ0mVEiB6NEDgZpeb2eXd+sY+cTAYam1iLSuptdL++iYW+QYcG5zFITLQtYUYJ7G9h1PnBI9YOC1H5rLVveTebVHC++hYV4DLDMTiKHgrQcCtLySj27HtlO3frh+UvkMpjSfGoS+vHjVD3TQjR5g8VY4MXnga4TEcL7I4T3N1H/k0pqnvsBKxan2roI0/Cj3UTxsuKRFRMOBqyDDTz+zRqaOpJ1/gIfvgIPBhbmyQiRD1qJfNBK4ws7WfF3O9m0VAF6ujH317Lhh62JPwwP3gU+fM6Bd2PETJPokRDRIyGCbzdQ/8YGdj6/hpIZbCBHflabCAaWrGHnixsILJj4McJ7d9Cwf+T3rG6TaLdJtCNM8O0GdrxUzc4X1uHXhWr2sVP0q+1320UFBXZBQYFd9JW77Yf+z232y++8Yz9zf4FdUPAV+4l/TvVII+tqfMT+UkGBXfTN3XZXKjvs+679lYIC+ys1+y7tg1MwWto6Q+/Y+97/yO48M8a+h/fZL/zV0sS5W7LUfiY0+N3f2C//xZfsguR5/dKdD9lPbHnZfuffnrMfKiqwC4oesXfHpulLyaRNuCx0vWU/9pXE9gVLvmTf/o3H7O//w1v2L/+fx+wvFRTYBStesDtH/KQu+52/+kpiv//6kL3tHz+yu3qHb/KR/c6zD9lfWZL87HeUYeRym9p82tn4iP2VggK7oOBL9gM1L9u//HTQtr1d9m/++QX7sTuLEp/3J9+3941R/w5JQtNj9lcKCuyiFc/Zv5noV4z90v7+nyTr6W98394duvgqFYvss597NFnX3/yIvXvkQi2/yz79pf3Ovl/Zv+kcox7+7Uf2O3//iH37kgK7oKDIXvkPE86NUyhmv/W/J8rSyn+YfIaN/bbT7hrtK5/psn/z4T5797OP2LcXJcrQ0i2/mvRnyfRJOSDo+sdn7Mc2vWC/Fey0L/zuH9nP3DkVAUGn/fI3i+yCgiL7kddSCgds+x8Tjaml/326M9Yk0jZc76+S56nAfmBYofvl3z9hf/fvd9v7Dg86ducL9solCghmqwmXhd5f2S/8zXftbbvesT/67YWXY28+kmg8jBYQfPqCvbKowC4oesDe9uFYKYrZv9y4NBGg/GWKAbXIVJnKfBr7pf39ryYaSndv+ZU9avXXtc/+bnK7B/7+o/HT2PuRve2eokkHzQM3hQr+5Pv2L8cKQHo/srfdX5Ss62eyoSezW8ze9zdfOV8WZu4y32W//BeJcvRY0/R/2kfb704E8vdPIiiXaZfyLEOeO9ax9YnVVJR6L/SDj09R/5boOzS9a4GnjDvvSO05kpXsMmR4pvm50yTSdhFXCSVfSJw180TnkLf8D2+i5uFKAtcPOnYq4yJkxky4LLhKWP10DdUPlFE4+HFs39ifY+5vIWiBcdtyVi8Za0sD/70VFLrAeq81tXE1IlNkKvOp1bqL3UcA7z2se7hk9DFXngDrHq/Ag0XwlcZx87y5t5b6DyyMW6pZWzb8qFEaHr6F0rtHG5NmEXy3FRPwP7AKf84YH+QqZMXdfgBC+9uY2Cg1SR8GJTcmOq1ZZtcsyCcGOMff6lIV3hbA5wJME1PXqVlnVkw7Gt3bSKsFntvuoSzFNrdpmliAJ8c97raXO20XSwyuAfDkqOOcpCZyqAML8BWVjD8uYLGf4jmAGSGc4tAYkakwlfk0vL+VKOBdWknZWA1vwFOerJOPtNLaMcaG8TD1P2okio8Vj6zAd9H7nRwKRzGTA4Qv1kn4YxPwUlh80d4X8d5Ugg+wPg4TUaNHRtF1NjkWJccYe7KR3yWGJ1FHGAbGTA+mlovMgp8kylt7glh4qPhaWcoDImPdFmBgzJnOBvbk0naReJDgBxbgoXDJ+BcUEQDzZBcA3gW542/s9OLLA34dJXocyJ/WpImcN3X51CR0IAIY+L9cMv6xcvz4i6Fxf4jQQQtGmdnH/PkWdrSDcVsVVbeNtE0HnceB+SO8BRA3MU3ANRdvKgMu8xbidUHkeBTF5jIyk/a2MAC+JSkE0iOyiL63m/pdTbS2hQgfMzG7rcRg5jwfhTcFqLh7FSvuuHh2w+DTZSx/PjIkPY0PL6Zx2Ha+h3bR8qR/UqkbMcWRDiJx8CwuvDgwlxk38wFBx24a37NgQQV33p56nGyaiYdsnpxpjK0nmbbhzOYG3joKeO+k4stTlzyR81wePDlAvIto10wnRmQUY+XTeIRDEcCVi+/6VJpIXnz5HsCk4+NOGKmJEQ+xY3sTJj5W/+UITwcATnZN7exuOXPxOIHuLsw+ZsNVVmabo2/R8AsTXD7K7kgh+L1IlKbvrOLxF8OJp1qGB9/CQnweoNskciRE65EQrW/UU7eshh2bV1A4qAnj/XIlqy0T6CK0t5FgFHy3r6Bs4dB2jufLUzlTVpSmV97CxEPFHYH0eSryOTLjVVVkbxPBOHhuv2fcR8SDWd1dgEVb3VrWvk5i3n+ngXt+Ll7fYkpu8hP4UiGeS/iGk03b0ISG2LG1kSgGJauqJn8cSTue+XOBKNFoJ4y7LIwbIwfAJHZ22pMmct7U5dMo5kmAhSy8OrXP9l6dC5hET0QZKSAwf15L/UEwbl9LdWCUJsh4Y+FcuXg9JIKYVKYYnpPsAtJnEesGtXxkKJOWbdtp6U48tVpVOvEjRJ5fmwgGjEIq/7qGdQ8Ghq7JEY8S+lktNU/XE9yzgeprC2l8wn8+K/qWrqNmaSItDR2NBH/hoXTlJmruuPRvdxHLJPJBC411W6jda+JdtpUNd6vr9Gw0wwFBhKbGIODlzj+dWMToLSzE44oSea+JyKgblVD5cA1/85B/EqvsTT5tF1gEn93AjoPA4tXU/MWEl8GRNFb4hcUYhIl8ECJKyTh5OIYVS/yfpX7LchlNWT7tM+lK3O5MBg3jM+Ymauau0yMMyxx4OuDysWbtPZew0qqXwi944b0ooQ/CcNs49bg1MBbBwhpn4gBJP2bLJmpeiUCOn3VPrp541xmrlbrnE12Zy76zg60PjnAEl5eSB2qom9/F0ocbCb+8g6a126mc5nZ4eFslFVtDI7+Z72f107Wse3CyXaRkus3soOJDjTQeALxfpWK0uzej8K3cSduHhzh0KPGv/f022t5toemVnWx/egNrlpXgORmi8enlLH+yZeJ9OS8hbQPMt2tY91wIK6eEdc+sG3t2CpFhjFvKCOSAtb+OHW1j3MXsDtGwfg1b3kv+3afV7eTymbJ8apFoSDsN3ClWuYbTOL/vcNGfbaH+IHj+aC1rbr602/T+27+KFwi9VEfLGE8JrI+bqPnz79IykB4F5zJYRwNPPNFAJO6h7K+3sGbMWblG8UELLUeBBWWsvnfscMJz+xpWLAbMVt7ZP/3XhblLyqi8u3LIv4qlAfyLPRhHg9R/v5rHt15YcFBmlxkNCMI/byIUT8woMck293lGjgdPvo/C0gAVD65hw7ON/OIf1lCSA5GXaqhtnVhhuNS0We07WLu+gUjcS8Xf1lJ9o54bywR576H6G4UQD7Pj4eVs+EkLoQ4zcWfVMokebKVx2+Msv6OSmtBiyoqT+zmV1+Qymo35NB5kx49bMF2FrPjWpTwdSDBur2JNqQEdDaxd+Tg79gSJnEg+BzCjhN9rZMd3VnH7srW0eAPJ1eyZ8WfwMouYQbY8UkNTFHwPbmXryskNq41+GEr0iij2j7/CsasQ/02JcTaHDo7al2LKeJeuY+vfbR3yb/uPdrJrbxv7f1ZD5dWdtGxbS9WzozxFkBk1c9VVPETjmyHAy51f809LN0tPYAM1/62J5c9HeOvnQWoCgcuTto5GnnhkE62mgf/xOraOE8WLjMzAv34HW4+v4YnXQzR8r4qG7128jW/pBnZ+7x5C65toAk3nJpfZFOVTgwt97xOTyI3LGnjKMGzb6M9qaTgEnjuSDflL5SpkzbPb6Xjkceo/aGTTtxrZdNE2XgLfrGPrIxbf/XITYGBchrnd5XPAClP/WBW17Rae22vY8beTn7Ww82iiv4PH60sh0DXw5iXG2ST2m7luy54bV7N1m8mhe7cQemEHTVVbqVDfoVll5poOB5to+jWw8E4qvzx9d4r8fxTA83wkGVUHUuuvdylpO9FKzTefoLHDoHDlduoeHWNxHZHxuHxUbm4i8OdNvPWPrQR/3YlpWRhzcsm9rphAxT1UFHuACMEuAA9zr5zhNEv6mYp86vQw1wDOmljdkEqLyepKjB2Ye+XgjSO8tasFEwgku/pMifwyan72C1a9vZumd9sJH+vE7DMwPLkUFvsp+1oF/nwDzIbE2AGnwVx1ExWiND25hppfmBil1dQ9u5rCS2h5Wd3J9QuuTK1l4fEkF0XtnvnlzyiupPLGLYTaWmgJQkXZTCdIBpuxgCDU2Eg4Dr6llfinMxVXech1JaYpNeOk9I0nnTYrxI5vraX+oIV32VZ2fOcS1i4QGcR7YwWrb6xg9WgbxKNETwKuuXiuuowJExnk0vKpF48XOBKlI8W1NKLHB9ZAGNzsd2O4DcCi9YfLqdyzcOx6+ESI1jjQ0cCGPw/hBYxANXWPjvRE2UPhHaspHGs2lmhy/QFPbmL6UUljJq1PV/H46xGMxavZ/qMZGEs4qwa2+yi83gNtJpEjUcaflUwup5kJCOJBGvdGAB8Vy6Zu0YuRP+vCbBYpxdOTTVs8QsNjVWzab+K5ZQN1mysTS3SLXA5nOxIrvzpTn7JR5LIbK5+6fCxeCBzqJPKxCTeOdzslSuSoCRgsvH7womhe7lm/gZbOLTQdjBDan2rfaZPIByG65s8l91LW8kguvsTChSzUNSCNWYSeX8va50NY+RVser6Gsilo/xrJtZes06mNizSTTxQ8ObPk9mQySDbPdqGAYHaZmerq3xtpOgJcV0HljdP7UVa0k8QTai9zU/m2k0qbScv31lCzN4pRvIbtP1oz/mAfkakUChKygOsKKZkl9b7IRcbMpx5KlvjgFxGC/xaCu8cZ89UdJNgOuEooKR5a4Ro3rmb7z0d9TjHU0XpWldXQmreGnW9vuOQn1uG2ICZajTXdRV5/nKoftmJ6Amz48VZWLJya4+bme4EwZjTC+PfYLaLHOgftN9NMzJPJLk+GGkmzzYzMMtT65ltEgMJllZRMc0gSCrYlKufrUxmAM5m0WQS3VbH2xTDWdSvY+uMNBNQgk8vKovXnbyUuDoHAJfVPFZk+4+fTwlsCeIHo3kZausc+mtm8mxYTKAgQyJuG5E5GPETj3jBgELhlmp9+y6xlttSw5skmos4S1mzfzpriqWv8er9Qkgg025PB9VjiYYLvm4CHxUtmQXhqBWl93wI8LFyYO+7mcnld/oDAaqXp/42Cq5CKZWMv2R1paaD+J4lltSelo4HaVyOAh8BtKVTOE0jbgPBLa1m7NYjlLWPTc5uoSKHfq8hUsj6oZdMriZVaJ7+Insj0SiWfGrct557rgOhutvw4NNLyAgknWtiytQkTA/+9039jKVXhFzdRfxBYUME9d+jOUDqyPqil6vF6wn0+KjfXsWGq7xDeWEYgHzjRQv3rY3eHM3+xk4ZDQE6Ar17iWhxTIfyTWnZHAaOEwFTM/iVT6rIHBFZrI28dBQoqqCweY8N4iJ3f20DN9x5ny95hEcHRVhpeaqSlNUQkOsIlw4wQfH0Tqx7cQMsJMG5cQ/XXxi+UKactKbp3A2u+20LU42fdc9sTC4CIXC5mmNafbGD5n9USssBzx1qqL3VBD5GpNpF86vKzZn0lXixC21ax6m8bCHYMquPjJuGWHTy+ci31R4DFq1m3cuZXgLc6gjQ8vYpV329NBCnfrNaUiuno4wYe/+YWgqY3sYrwsmnopmMEqHrIj5Hsqvz4i61EhjeD4lFCr9dQtb6BKFD4jTVULJj6pAx8Vrg9fH5djovfN4m2t1D/neUsfyaIBXjvXsOds6EHkwxxme+rWLT+/B2iQMldlePMiDv4IjDsnbZ6ap5surCFy8Az35OY17rbJGpe2NdYsoKt26pTuIM0kbQB8RA7ftCQGDxmBtnyQDFbxtsnkSIqfrCf7ffqaiHjM994nMpn2y+az9w6GRkSDHtuWUfd5hUaoiUzYirzqXfZD6jrMKl6poXgCxtY/sIGDI8XTw5YJ6IMVO/GdZXU/Ggdgcs0a0vr31awYf+wySn6LLqinYOuOR5K/mwr2x+e+SBFLr+WH22iKQoQpeV7FSy+aD2OkRh4v76d/T8sS/lzCh/aztZDq3j8pTCN31lF4/c8eK/LJXeOAWdNIh2RZDlJrP9R+/j0rPUEQMdu1t27idBAO80whqwxMjBN6gDPLevY/qRmYJyNLm9A0N3K7uYouEqo+No4FabLi9drwCdG4r+DGLeso+7ZMkLt7YTaQ7R/HKHzuEm020oEB14fvi+UUva11ax+wI83lW85kbQBYGKdHfRnyktxW4k5qkVS0IUbL12Ej1xoCAEYHg++xSUsLvYTWLacFUsLVcHKjJnafGpQ8nAde8uaqN/ZQMv+EOGOKNGzYMzxUljqJ3DHCqpWluG7bA/ETCzDi3E6RCRqnp+5DpeB4fFQeGMhi28KcOfyFVTeqLA8PVmYZ0e/kTnWfhNvE3ipeLqRX/zpbup/1kRrW4hwRzgxpsDw4M0rIXBTgIq7l7PijsLp7UaaU8Lyv1xDyScROiMddERNus52YZ21EjMcGYk2WWGxn7Jlq1h9d4muVbOUw7Zte6YTISIiIiIiM2NGZhkSEREREZHZQQGBiIiIiEgaU0AgIiIiIpLGFBCIiIiIiKQxBQQiIiIiImlMAYGIiIiISBpTQCAiIiIiksYUEIiIiIiIpDEFBCIiIiIiaUwBgYiIiIhIGlNAICIiIiKSxhQQiIiIiIikMQUEIiIiIiJpTAGBiIiIiEgaU0AgIiIiIpLG/n9up5dhdLJwsAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Current Ranking (baseline)\n",
    "Image(\"img/current-25-05.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard\n",
    "# https://tensorboard.dev/experiment/rsdMubj0S165iOdLmbd13A/#scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sj_model.save('models/sj_model_17.57MAP.h5') \n",
    "# iq_model.save('models/iq_model_17.57MAP.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
